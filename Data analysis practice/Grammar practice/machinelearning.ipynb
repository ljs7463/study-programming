{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow'\n"
     ]
    }
   ],
   "source": [
    "#Create a constant op\n",
    "#This op is added as a nod to the default graph\n",
    "hello = tf.constant(\"Hello, TensorFlow\")\n",
    "\n",
    "# #seart a TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# # run the op and get rulest\n",
    "print(sess.run(hello))\n",
    "\n",
    "#그래프를 실행하기 위해 세션을 만들고 sess.run 함수로 실행\n",
    "#b는 나오는거 걱정 x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) #also tf.float32 implicitly\n",
    "node3 = tf.add(node1, node2) #node3 = node1 +node2 이렇게도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_2:0\", shape=(), dtype=float32) node2: Tensor(\"Const_3:0\", shape=(), dtype=float32)\n",
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"node1:\", node1, \"node2:\", node2)\n",
    "print(\"node3:\", node3)\n",
    "# 이렇게 그냥 출력하면 출력의 결과는 '이것은 이러한 형태의 그래프야'\n",
    "# 라고 출력이 된다.(애초에 그래프니깐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sses.run(node1, node2): [3.0, 4.0]\n",
      "sess.run(node3): 7.0\n"
     ]
    }
   ],
   "source": [
    "#그래서 결과값을 나오게 하려면\n",
    "sess = tf.Session()  #우선 세션을 만든다\n",
    "print(\"sses.run(node1, node2):\", sess.run([node1, node2])) #그리고 sess.run으로 그래프를 실행시킨다.\n",
    "print(\"sess.run(node3):\", sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 요약하면\n",
    " 1. 그래프를 빌드해주고\n",
    " 2. sess.run() 세션을 만들고\n",
    " 3. output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b # + provides a shortcut for tf.add(a, b)\n",
    "\n",
    "print(sess.run(adder_node, feed_dict= {a:3, b:4.5}))\n",
    "print(sess.run(adder_node, feed_dict= {a:[1,3],b:[2,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tensor Ranks, Shapes, and Types\n",
    "#Ranks(=몇차원 array)\n",
    " Ranks shape\n",
    " 0차원  []                 : Scalar    / s =483\n",
    " 1차원  [5]                : Vector    / v =[1.1, 2.2, 3.3]\n",
    " 2차원  [3,4]              : Matrix    / m =[[1,2,3],[4,5,6],[7,8,9]]\n",
    " 3차원  [1,4,3]            : 3-Tensor  / t =[[[2], [4],[6], [8], [10],[12], [14], [16], [18]]]\n",
    " n차원  [DO,D1,..Dn-1]     : n-Tensor  / ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.288587 [0.38785022] [-1.7821549]\n",
      "20 0.2914691 [1.4200197] [-1.2570025]\n",
      "40 0.19087256 [1.4958235] [-1.1558986]\n",
      "60 0.172684 [1.4816192] [-1.0975736]\n",
      "80 0.15682818 [1.459851] [-1.0456105]\n",
      "100 0.14243387 [1.4383224] [-0.99643433]\n",
      "120 0.12936077 [1.4177306] [-0.9496022]\n",
      "140 0.1174875 [1.3980995] [-0.90497404]\n",
      "160 0.106703974 [1.3793902] [-0.86244345]\n",
      "180 0.096910246 [1.3615602] [-0.82191175]\n",
      "200 0.08801544 [1.3445683] [-0.7832849]\n",
      "220 0.07993702 [1.3283749] [-0.7464733]\n",
      "240 0.07260007 [1.3129425] [-0.71139175]\n",
      "260 0.06593653 [1.2982353] [-0.677959]\n",
      "280 0.059884656 [1.2842194] [-0.6460974]\n",
      "300 0.054388206 [1.2708621] [-0.61573327]\n",
      "320 0.049396235 [1.2581326] [-0.58679605]\n",
      "340 0.044862453 [1.2460012] [-0.55921894]\n",
      "360 0.040744774 [1.2344402] [-0.5329376]\n",
      "380 0.037005108 [1.2234225] [-0.5078916]\n",
      "400 0.03360861 [1.2129223] [-0.48402247]\n",
      "420 0.030523876 [1.2029157] [-0.46127522]\n",
      "440 0.027722282 [1.1933795] [-0.439597]\n",
      "460 0.02517782 [1.1842916] [-0.4189376]\n",
      "480 0.022866897 [1.1756303] [-0.39924908]\n",
      "500 0.020768084 [1.1673764] [-0.38048586]\n",
      "520 0.018861907 [1.1595104] [-0.3626044]\n",
      "540 0.017130671 [1.1520138] [-0.34556332]\n",
      "560 0.015558372 [1.1448698] [-0.32932314]\n",
      "580 0.014130354 [1.1380614] [-0.3138462]\n",
      "600 0.012833409 [1.1315731] [-0.29909652]\n",
      "620 0.011655512 [1.1253896] [-0.28504008]\n",
      "640 0.010585719 [1.1194967] [-0.2716443]\n",
      "660 0.009614115 [1.1138808] [-0.2588779]\n",
      "680 0.0087317 [1.1085289] [-0.24671158]\n",
      "700 0.0079302685 [1.1034284] [-0.23511703]\n",
      "720 0.0072023966 [1.0985676] [-0.2240673]\n",
      "740 0.0065413197 [1.0939354] [-0.213537]\n",
      "760 0.0059409407 [1.0895207] [-0.20350157]\n",
      "780 0.0053956583 [1.0853134] [-0.19393772]\n",
      "800 0.0049004215 [1.0813041] [-0.18482336]\n",
      "820 0.0044506267 [1.077483] [-0.17613725]\n",
      "840 0.004042132 [1.0738416] [-0.16785944]\n",
      "860 0.003671137 [1.0703713] [-0.15997063]\n",
      "880 0.0033341805 [1.067064] [-0.15245259]\n",
      "900 0.0030281574 [1.0639124] [-0.14528784]\n",
      "920 0.0027502247 [1.060909] [-0.13845995]\n",
      "940 0.0024977948 [1.0580462] [-0.13195285]\n",
      "960 0.002268537 [1.0553184] [-0.12575154]\n",
      "980 0.0020603212 [1.0527185] [-0.11984168]\n",
      "1000 0.0018712204 [1.050241] [-0.11420959]\n",
      "1020 0.0016994724 [1.0478798] [-0.10884218]\n",
      "1040 0.0015434903 [1.0456296] [-0.10372698]\n",
      "1060 0.0014018213 [1.0434853] [-0.09885219]\n",
      "1080 0.0012731581 [1.0414416] [-0.09420655]\n",
      "1100 0.0011562934 [1.0394938] [-0.08977903]\n",
      "1120 0.0010501697 [1.0376378] [-0.08555968]\n",
      "1140 0.00095377467 [1.035869] [-0.0815387]\n",
      "1160 0.0008662369 [1.0341833] [-0.07770666]\n",
      "1180 0.0007867285 [1.0325768] [-0.07405473]\n",
      "1200 0.0007145161 [1.0310458] [-0.07057445]\n",
      "1220 0.000648935 [1.0295867] [-0.06725767]\n",
      "1240 0.00058937696 [1.0281963] [-0.06409678]\n",
      "1260 0.00053528085 [1.0268712] [-0.06108449]\n",
      "1280 0.0004861496 [1.0256083] [-0.05821373]\n",
      "1300 0.00044152758 [1.0244049] [-0.05547792]\n",
      "1320 0.00040100573 [1.023258] [-0.05287069]\n",
      "1340 0.00036419972 [1.0221651] [-0.05038599]\n",
      "1360 0.00033077647 [1.0211235] [-0.04801816]\n",
      "1380 0.00030041338 [1.0201305] [-0.04576156]\n",
      "1400 0.00027284122 [1.0191844] [-0.04361092]\n",
      "1420 0.00024779674 [1.0182829] [-0.04156129]\n",
      "1440 0.00022505324 [1.0174236] [-0.03960804]\n",
      "1460 0.0002043952 [1.0166048] [-0.03774658]\n",
      "1480 0.00018563734 [1.0158244] [-0.03597264]\n",
      "1500 0.00016859891 [1.0150807] [-0.03428205]\n",
      "1520 0.0001531234 [1.014372] [-0.03267087]\n",
      "1540 0.00013906749 [1.0136966] [-0.03113548]\n",
      "1560 0.00012630329 [1.0130528] [-0.02967217]\n",
      "1580 0.00011471147 [1.0124395] [-0.02827769]\n",
      "1600 0.00010418435 [1.0118548] [-0.02694876]\n",
      "1620 9.462042e-05 [1.0112977] [-0.02568227]\n",
      "1640 8.593633e-05 [1.0107667] [-0.02447532]\n",
      "1660 7.80493e-05 [1.0102608] [-0.02332508]\n",
      "1680 7.088355e-05 [1.0097784] [-0.02222889]\n",
      "1700 6.437768e-05 [1.0093188] [-0.02118412]\n",
      "1720 5.8468217e-05 [1.008881] [-0.02018849]\n",
      "1740 5.3102554e-05 [1.0084636] [-0.01923969]\n",
      "1760 4.822922e-05 [1.0080659] [-0.01833555]\n",
      "1780 4.3802775e-05 [1.0076869] [-0.01747391]\n",
      "1800 3.9782117e-05 [1.0073256] [-0.01665271]\n",
      "1820 3.6131023e-05 [1.0069814] [-0.01587013]\n",
      "1840 3.281448e-05 [1.0066532] [-0.01512434]\n",
      "1860 2.9802546e-05 [1.0063405] [-0.01441348]\n",
      "1880 2.7068143e-05 [1.0060426] [-0.01373609]\n",
      "1900 2.4582581e-05 [1.0057585] [-0.01309054]\n",
      "1920 2.2326967e-05 [1.0054879] [-0.01247531]\n",
      "1940 2.0277641e-05 [1.0052301] [-0.01188903]\n",
      "1960 1.8416236e-05 [1.0049843] [-0.01133035]\n",
      "1980 1.672629e-05 [1.00475] [-0.01079783]\n",
      "2000 1.51911045e-05 [1.0045267] [-0.01029034]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "X_train = [1,2,3]\n",
    "Y_train = [1,2,3]\n",
    "\n",
    "w = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = X_train * w + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-Y_train))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01) \n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())  # 앞에서 Variable 해줬기 때문에 사용하는 코드\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train) #train을 빌딩한다 = 밑에있는 cost,hypothesis, w, b 값 모두 빌딩하는것\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(w), sess.run(b))\n",
    "        \n",
    "          \n",
    "# h(x) = wx + b 이니깐 w는 1이 b는 0이 나와야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with placeholers\n",
    "(learning_rate 가 뭔지 설명하는 글 https://aileen93.tistory.com/71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 83.31692 [-2.8915806] [0.31014252]\n",
      "20 1.1274366 [-0.00984592] [1.4827932]\n",
      "40 0.34810197 [0.2945952] [1.5261543]\n",
      "60 0.31002393 [0.35221636] [1.4651948]\n",
      "80 0.28151318 [0.38498986] [1.3973608]\n",
      "100 0.25567412 [0.41411495] [1.3317875]\n",
      "120 0.23220737 [0.44167057] [1.2692076]\n",
      "140 0.21089439 [0.46791208] [1.2095604]\n",
      "160 0.19153778 [0.49291837] [1.1527158]\n",
      "180 0.1739577 [0.5167493] [1.0985425]\n",
      "200 0.15799119 [0.5394603] [1.0469149]\n",
      "220 0.14349012 [0.56110394] [0.9977139]\n",
      "240 0.13031997 [0.58173054] [0.950825]\n",
      "260 0.11835876 [0.60138756] [0.90613973]\n",
      "280 0.10749525 [0.62012094] [0.8635544]\n",
      "300 0.09762892 [0.63797385] [0.8229705]\n",
      "320 0.08866813 [0.6549877] [0.7842939]\n",
      "340 0.080529876 [0.671202] [0.7474351]\n",
      "360 0.073138565 [0.68665427] [0.7123085]\n",
      "380 0.06642561 [0.7013803] [0.67883277]\n",
      "400 0.06032877 [0.71541446] [0.64693004]\n",
      "420 0.05479158 [0.7287889] [0.61652666]\n",
      "440 0.049762577 [0.74153477] [0.58755225]\n",
      "460 0.045195136 [0.75368166] [0.55993944]\n",
      "480 0.041047003 [0.76525766] [0.5336243]\n",
      "500 0.037279528 [0.77628976] [0.50854594]\n",
      "520 0.03385784 [0.7868034] [0.4846461]\n",
      "540 0.030750254 [0.79682285] [0.46186954]\n",
      "560 0.02792786 [0.8063714] [0.44016334]\n",
      "580 0.025364535 [0.8154712] [0.4194773]\n",
      "600 0.023036465 [0.82414335] [0.3997634]\n",
      "620 0.020922074 [0.832408] [0.380976]\n",
      "640 0.019001765 [0.8402842] [0.36307147]\n",
      "660 0.017257717 [0.8477903] [0.34600845]\n",
      "680 0.015673736 [0.8549436] [0.32974732]\n",
      "700 0.01423514 [0.8617607] [0.31425047]\n",
      "720 0.012928563 [0.8682575] [0.29948184]\n",
      "740 0.011741941 [0.8744489] [0.28540727]\n",
      "760 0.010664222 [0.8803493] [0.27199417]\n",
      "780 0.009685413 [0.88597244] [0.25921145]\n",
      "800 0.00879646 [0.8913313] [0.24702948]\n",
      "820 0.007989076 [0.89643836] [0.23541999]\n",
      "840 0.007255807 [0.90130544] [0.22435606]\n",
      "860 0.00658983 [0.9059437] [0.21381214]\n",
      "880 0.005984992 [0.910364] [0.20376378]\n",
      "900 0.00543567 [0.91457653] [0.19418763]\n",
      "920 0.0049367584 [0.918591] [0.18506156]\n",
      "940 0.0044836495 [0.92241704] [0.17636436]\n",
      "960 0.004072121 [0.9260631] [0.16807587]\n",
      "980 0.003698362 [0.9295379] [0.16017695]\n",
      "1000 0.0033589154 [0.93284935] [0.15264921]\n",
      "1020 0.003050617 [0.9360051] [0.1454753]\n",
      "1040 0.002770625 [0.9390127] [0.13863848]\n",
      "1060 0.0025163286 [0.94187886] [0.13212296]\n",
      "1080 0.0022853634 [0.94461036] [0.12591371]\n",
      "1100 0.0020756088 [0.9472135] [0.1199962]\n",
      "1120 0.0018850978 [0.9496943] [0.11435683]\n",
      "1140 0.0017120759 [0.95205843] [0.10898246]\n",
      "1160 0.0015549347 [0.95431155] [0.10386069]\n",
      "1180 0.0014122146 [0.95645875] [0.09897958]\n",
      "1200 0.0012825967 [0.95850503] [0.09432789]\n",
      "1220 0.0011648732 [0.96045524] [0.08989476]\n",
      "1240 0.0010579561 [0.96231365] [0.08566999]\n",
      "1260 0.0009608534 [0.96408474] [0.08164378]\n",
      "1280 0.00087266136 [0.9657726] [0.07780684]\n",
      "1300 0.00079256506 [0.9673812] [0.0741502]\n",
      "1320 0.00071981904 [0.96891415] [0.07066545]\n",
      "1340 0.0006537521 [0.9703752] [0.06734437]\n",
      "1360 0.0005937437 [0.97176737] [0.06417938]\n",
      "1380 0.00053924933 [0.9730942] [0.06116319]\n",
      "1400 0.00048975734 [0.97435874] [0.05828872]\n",
      "1420 0.000444805 [0.9755637] [0.05554936]\n",
      "1440 0.00040397758 [0.97671217] [0.05293875]\n",
      "1460 0.00036690128 [0.97780657] [0.05045083]\n",
      "1480 0.0003332251 [0.97884953] [0.04807985]\n",
      "1500 0.00030263994 [0.97984374] [0.04582025]\n",
      "1520 0.00027486257 [0.98079085] [0.04366682]\n",
      "1540 0.00024963467 [0.9816937] [0.04161465]\n",
      "1560 0.00022672181 [0.98255396] [0.03965889]\n",
      "1580 0.00020591129 [0.9833739] [0.03779507]\n",
      "1600 0.00018701209 [0.98415524] [0.03601885]\n",
      "1620 0.00016984683 [0.9848998] [0.03432615]\n",
      "1640 0.00015425688 [0.98560953] [0.03271295]\n",
      "1660 0.0001401003 [0.9862858] [0.03117556]\n",
      "1680 0.00012724088 [0.9869303] [0.02971044]\n",
      "1700 0.00011556249 [0.9875446] [0.02831415]\n",
      "1720 0.00010495505 [0.9881299] [0.02698347]\n",
      "1740 9.532202e-05 [0.98868775] [0.02571535]\n",
      "1760 8.65733e-05 [0.98921937] [0.02450684]\n",
      "1780 7.862782e-05 [0.98972595] [0.02335512]\n",
      "1800 7.141024e-05 [0.990209] [0.02225753]\n",
      "1820 6.485546e-05 [0.9906691] [0.02121146]\n",
      "1840 5.8903224e-05 [0.9911076] [0.02021458]\n",
      "1860 5.3497515e-05 [0.99152553] [0.01926454]\n",
      "1880 4.8586353e-05 [0.9919238] [0.01835915]\n",
      "1900 4.4126315e-05 [0.9923034] [0.01749631]\n",
      "1920 4.0076342e-05 [0.99266505] [0.01667403]\n",
      "1940 3.6397676e-05 [0.9930098] [0.01589042]\n",
      "1960 3.3057815e-05 [0.9933383] [0.0151436]\n",
      "1980 3.002324e-05 [0.9936514] [0.01443192]\n",
      "2000 2.7267799e-05 [0.9939498] [0.01375367]\n"
     ]
    }
   ],
   "source": [
    "# Placeholders를 이용해보기\n",
    "import tensorflow as tf\n",
    "# X_train = [1,2,3]\n",
    "# Y_train = [1,2,3]\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "w = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = X * w + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) # 앞에서 Variable 해줬기 때문에 사용하는 코드\n",
    "\n",
    "# for step in range(2001):\n",
    "#     sess.run(train)\n",
    "#     if step % 20 == 0:\n",
    "#         print(step, sess.run(cost), sess.run(w), sess.run(b))\n",
    "for step in range(2001):\n",
    "    cost_val, w_val, b_val,_ =\\\n",
    "        sess.run([cost, w, b, train],\n",
    "                feed_dict = {X: [1,2,3], Y: [1,2,3]})\n",
    "    if step % 20 == 0:\n",
    "        print(step, cost_val, w_val, b_val)\n",
    "\n",
    "          \n",
    "# h(x) = wx + b 이니깐 w는 1이 b는 0이 나와야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사도를 구하는 것을 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "hypothesis = X * W\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "W_val = []\n",
    "cost_val = []\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i * 0.1\n",
    "    curr_cost, curr_W = sess.run([cost, W], feed_dict = {W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "    \n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.6227012 [0.56717694]\n",
      "1 0.7460127 [0.76916105]\n",
      "2 0.21219918 [0.8768859]\n",
      "3 0.060358785 [0.93433917]\n",
      "4 0.017168712 [0.9649809]\n",
      "5 0.004883574 [0.9813231]\n",
      "6 0.0013890967 [0.990039]\n",
      "7 0.0003951302 [0.99468744]\n",
      "8 0.00011239151 [0.99716663]\n",
      "9 3.197086e-05 [0.99848884]\n",
      "10 9.094893e-06 [0.999194]\n",
      "11 2.5870193e-06 [0.99957013]\n",
      "12 7.355463e-07 [0.99977076]\n",
      "13 2.0918417e-07 [0.99987775]\n",
      "14 5.9481593e-08 [0.9999348]\n",
      "15 1.696344e-08 [0.9999652]\n",
      "16 4.8040825e-09 [0.99998146]\n",
      "17 1.3635173e-09 [0.9999901]\n",
      "18 3.9587533e-10 [0.9999947]\n",
      "19 1.0887291e-10 [0.9999972]\n",
      "20 3.1622704e-11 [0.9999985]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X *W\n",
    "\n",
    "# Cost/Loss function\n",
    "cost = tf.reduce_sum(tf.square(hypothesis-Y))\n",
    "\n",
    "# Minimize: Gradient Descent using derivative: W  -= Learning_rate *derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y)* X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "# Launch the grapg in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in thr graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict = {X: x_data, Y: y_data})\n",
    "    print(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}), sess.run(W))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariable linear regression(멀티베리어블 경우 실제 데이터를 적용해서 다뤄볼 수 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost:  4428.6885 \n",
      " Prediction:\n",
      " [ 98.93816  109.53171  113.08899  120.74727   82.903786]\n",
      "10 cost:  18.047108 \n",
      " Prediction:\n",
      " [157.67108 180.15533 182.6595  196.50983 136.77786]\n",
      "20 cost:  17.911236 \n",
      " Prediction:\n",
      " [157.8324  180.38019 182.86491 196.73558 136.95541]\n",
      "30 cost:  17.816364 \n",
      " Prediction:\n",
      " [157.81657 180.39204 182.86052 196.73282 136.97044]\n",
      "40 cost:  17.722023 \n",
      " Prediction:\n",
      " [157.80026 180.40321 182.85551 196.72937 136.98495]\n",
      "50 cost:  17.628138 \n",
      " Prediction:\n",
      " [157.78398 180.41435 182.8505  196.72592 136.99942]\n",
      "60 cost:  17.534653 \n",
      " Prediction:\n",
      " [157.76775 180.4255  182.8455  196.72249 137.01387]\n",
      "70 cost:  17.44181 \n",
      " Prediction:\n",
      " [157.75154 180.43657 182.84052 196.71906 137.02824]\n",
      "80 cost:  17.349358 \n",
      " Prediction:\n",
      " [157.73538 180.44762 182.83553 196.71564 137.04262]\n",
      "90 cost:  17.257513 \n",
      " Prediction:\n",
      " [157.71928 180.45863 182.83057 196.71225 137.05693]\n",
      "100 cost:  17.166134 \n",
      " Prediction:\n",
      " [157.70322 180.46964 182.82564 196.70885 137.0712 ]\n",
      "110 cost:  17.07521 \n",
      " Prediction:\n",
      " [157.6872  180.4806  182.8207  196.70546 137.08543]\n",
      "120 cost:  16.984848 \n",
      " Prediction:\n",
      " [157.67122 180.49153 182.81578 196.70209 137.09962]\n",
      "130 cost:  16.894878 \n",
      " Prediction:\n",
      " [157.65529 180.50246 182.81088 196.69872 137.1138 ]\n",
      "140 cost:  16.805477 \n",
      " Prediction:\n",
      " [157.63939 180.51332 182.806   196.69537 137.12791]\n",
      "150 cost:  16.716501 \n",
      " Prediction:\n",
      " [157.62354 180.52417 182.80109 196.69202 137.14198]\n",
      "160 cost:  16.628075 \n",
      " Prediction:\n",
      " [157.60774 180.53499 182.79625 196.68867 137.15604]\n",
      "170 cost:  16.540033 \n",
      " Prediction:\n",
      " [157.59196 180.54578 182.79138 196.68535 137.17004]\n",
      "180 cost:  16.452547 \n",
      " Prediction:\n",
      " [157.57626 180.55655 182.78654 196.682   137.184  ]\n",
      "190 cost:  16.365475 \n",
      " Prediction:\n",
      " [157.56056 180.56726 182.78171 196.6787  137.19794]\n",
      "200 cost:  16.278952 \n",
      " Prediction:\n",
      " [157.54494 180.57796 182.7769  196.67538 137.21182]\n",
      "210 cost:  16.192804 \n",
      " Prediction:\n",
      " [157.52934 180.58865 182.7721  196.67212 137.22568]\n",
      "220 cost:  16.107191 \n",
      " Prediction:\n",
      " [157.5138  180.59929 182.7673  196.66882 137.23949]\n",
      "230 cost:  16.022043 \n",
      " Prediction:\n",
      " [157.49828 180.60988 182.76253 196.66556 137.25327]\n",
      "240 cost:  15.937329 \n",
      " Prediction:\n",
      " [157.48282 180.62048 182.75778 196.66231 137.26701]\n",
      "250 cost:  15.853047 \n",
      " Prediction:\n",
      " [157.46738 180.63103 182.75302 196.65904 137.28072]\n",
      "260 cost:  15.769223 \n",
      " Prediction:\n",
      " [157.452   180.64157 182.74829 196.65579 137.29439]\n",
      "270 cost:  15.685869 \n",
      " Prediction:\n",
      " [157.43666 180.65208 182.74356 196.65256 137.30801]\n",
      "280 cost:  15.602961 \n",
      " Prediction:\n",
      " [157.42134 180.66254 182.73883 196.6493  137.3216 ]\n",
      "290 cost:  15.520513 \n",
      " Prediction:\n",
      " [157.40608 180.67297 182.73413 196.64612 137.33516]\n",
      "300 cost:  15.438497 \n",
      " Prediction:\n",
      " [157.39087 180.68341 182.72946 196.6429  137.34868]\n",
      "310 cost:  15.356941 \n",
      " Prediction:\n",
      " [157.37567 180.69377 182.72478 196.6397  137.36215]\n",
      "320 cost:  15.275808 \n",
      " Prediction:\n",
      " [157.36053 180.70413 182.72011 196.63649 137.3756 ]\n",
      "330 cost:  15.19517 \n",
      " Prediction:\n",
      " [157.34546 180.71448 182.71548 196.63333 137.389  ]\n",
      "340 cost:  15.114888 \n",
      " Prediction:\n",
      " [157.33038 180.72478 182.71082 196.63016 137.40236]\n",
      "350 cost:  15.035077 \n",
      " Prediction:\n",
      " [157.31538 180.73506 182.7062  196.62698 137.41571]\n",
      "360 cost:  14.955683 \n",
      " Prediction:\n",
      " [157.30038 180.7453  182.70158 196.62381 137.42899]\n",
      "370 cost:  14.876709 \n",
      " Prediction:\n",
      " [157.28546 180.75554 182.69699 196.62068 137.44226]\n",
      "380 cost:  14.798166 \n",
      " Prediction:\n",
      " [157.27055 180.76572 182.6924  196.61754 137.45549]\n",
      "390 cost:  14.720053 \n",
      " Prediction:\n",
      " [157.2557  180.7759  182.68784 196.61441 137.46869]\n",
      "400 cost:  14.642366 \n",
      " Prediction:\n",
      " [157.24089 180.78604 182.68326 196.6113  137.48183]\n",
      "410 cost:  14.565074 \n",
      " Prediction:\n",
      " [157.2261  180.79614 182.6787  196.60818 137.49495]\n",
      "420 cost:  14.488226 \n",
      " Prediction:\n",
      " [157.21136 180.80623 182.67416 196.60507 137.50803]\n",
      "430 cost:  14.411824 \n",
      " Prediction:\n",
      " [157.19669 180.81628 182.66963 196.60197 137.52107]\n",
      "440 cost:  14.335844 \n",
      " Prediction:\n",
      " [157.18204 180.82632 182.66516 196.5989  137.53409]\n",
      "450 cost:  14.260158 \n",
      " Prediction:\n",
      " [157.16739 180.8363  182.66063 196.59583 137.54707]\n",
      "460 cost:  14.184993 \n",
      " Prediction:\n",
      " [157.15282 180.84628 182.65614 196.59276 137.56   ]\n",
      "470 cost:  14.110179 \n",
      " Prediction:\n",
      " [157.13829 180.85625 182.65166 196.58969 137.5729 ]\n",
      "480 cost:  14.035776 \n",
      " Prediction:\n",
      " [157.12378 180.86617 182.64719 196.58664 137.58577]\n",
      "490 cost:  13.961821 \n",
      " Prediction:\n",
      " [157.10933 180.87605 182.64273 196.5836  137.5986 ]\n",
      "500 cost:  13.888216 \n",
      " Prediction:\n",
      " [157.09491 180.88594 182.6383  196.58057 137.6114 ]\n",
      "510 cost:  13.814998 \n",
      " Prediction:\n",
      " [157.08054 180.89578 182.63385 196.57753 137.62418]\n",
      "520 cost:  13.742228 \n",
      " Prediction:\n",
      " [157.06618 180.9056  182.62944 196.57455 137.63689]\n",
      "530 cost:  13.669835 \n",
      " Prediction:\n",
      " [157.05188 180.91537 182.62505 196.57153 137.6496 ]\n",
      "540 cost:  13.597811 \n",
      " Prediction:\n",
      " [157.03761 180.92514 182.62065 196.56854 137.66226]\n",
      "550 cost:  13.526217 \n",
      " Prediction:\n",
      " [157.02339 180.93488 182.61627 196.56555 137.67488]\n",
      "560 cost:  13.45495 \n",
      " Prediction:\n",
      " [157.00919 180.94458 182.6119  196.56256 137.68747]\n",
      "570 cost:  13.384097 \n",
      " Prediction:\n",
      " [156.99504 180.95427 182.60753 196.5596  137.70003]\n",
      "580 cost:  13.313609 \n",
      " Prediction:\n",
      " [156.98093 180.96393 182.60318 196.55661 137.71255]\n",
      "590 cost:  13.243533 \n",
      " Prediction:\n",
      " [156.96686 180.97356 182.59885 196.55368 137.72505]\n",
      "600 cost:  13.173795 \n",
      " Prediction:\n",
      " [156.9528  180.98315 182.59451 196.5507  137.7375 ]\n",
      "610 cost:  13.104459 \n",
      " Prediction:\n",
      " [156.93881 180.99275 182.59021 196.54778 137.74992]\n",
      "620 cost:  13.035497 \n",
      " Prediction:\n",
      " [156.92485 181.0023  182.5859  196.54485 137.76231]\n",
      "630 cost:  12.966942 \n",
      " Prediction:\n",
      " [156.91092 181.01181 182.58162 196.5419  137.77466]\n",
      "640 cost:  12.898707 \n",
      " Prediction:\n",
      " [156.89703 181.02133 182.57732 196.53899 137.78696]\n",
      "650 cost:  12.830847 \n",
      " Prediction:\n",
      " [156.8832  181.0308  182.57306 196.5361  137.79927]\n",
      "660 cost:  12.763368 \n",
      " Prediction:\n",
      " [156.86938 181.04027 182.56882 196.53319 137.81152]\n",
      "670 cost:  12.696238 \n",
      " Prediction:\n",
      " [156.85559 181.04968 182.56456 196.5303  137.82373]\n",
      "680 cost:  12.629466 \n",
      " Prediction:\n",
      " [156.84184 181.05908 182.56032 196.5274  137.8359 ]\n",
      "690 cost:  12.563074 \n",
      " Prediction:\n",
      " [156.82814 181.06845 182.5561  196.52454 137.84807]\n",
      "700 cost:  12.497075 \n",
      " Prediction:\n",
      " [156.8145  181.07782 182.55191 196.52168 137.86018]\n",
      "710 cost:  12.431381 \n",
      " Prediction:\n",
      " [156.80086 181.08713 182.5477  196.51881 137.87227]\n",
      "720 cost:  12.366045 \n",
      " Prediction:\n",
      " [156.78728 181.09644 182.54352 196.51598 137.88434]\n",
      "730 cost:  12.301084 \n",
      " Prediction:\n",
      " [156.77371 181.1057  182.53934 196.51312 137.89635]\n",
      "740 cost:  12.236406 \n",
      " Prediction:\n",
      " [156.7602  181.11497 182.53517 196.5103  137.90834]\n",
      "750 cost:  12.172195 \n",
      " Prediction:\n",
      " [156.7467  181.12416 182.53102 196.50748 137.92029]\n",
      "760 cost:  12.108248 \n",
      " Prediction:\n",
      " [156.73328 181.13339 182.52689 196.50467 137.93222]\n",
      "770 cost:  12.04469 \n",
      " Prediction:\n",
      " [156.71986 181.14256 182.52275 196.50185 137.94409]\n",
      "780 cost:  11.981448 \n",
      " Prediction:\n",
      " [156.70648 181.15169 182.5186  196.49905 137.95595]\n",
      "790 cost:  11.918516 \n",
      " Prediction:\n",
      " [156.69313 181.16083 182.51451 196.49623 137.96777]\n",
      "800 cost:  11.855959 \n",
      " Prediction:\n",
      " [156.67984 181.16994 182.5104  196.49345 137.97957]\n",
      "810 cost:  11.79376 \n",
      " Prediction:\n",
      " [156.66658 181.179   182.50632 196.49068 137.99133]\n",
      "820 cost:  11.731852 \n",
      " Prediction:\n",
      " [156.65334 181.18805 182.50224 196.48792 138.00307]\n",
      "830 cost:  11.670357 \n",
      " Prediction:\n",
      " [156.64015 181.19707 182.49818 196.48515 138.01476]\n",
      "840 cost:  11.609095 \n",
      " Prediction:\n",
      " [156.62698 181.20609 182.49414 196.48239 138.02643]\n",
      "850 cost:  11.548216 \n",
      " Prediction:\n",
      " [156.61386 181.21506 182.49008 196.47964 138.03806]\n",
      "860 cost:  11.48767 \n",
      " Prediction:\n",
      " [156.60078 181.22401 182.48604 196.4769  138.04965]\n",
      "870 cost:  11.427406 \n",
      " Prediction:\n",
      " [156.58772 181.23296 182.48201 196.47417 138.06122]\n",
      "880 cost:  11.367492 \n",
      " Prediction:\n",
      " [156.5747  181.24187 182.47801 196.47145 138.07277]\n",
      "890 cost:  11.307967 \n",
      " Prediction:\n",
      " [156.56172 181.25072 182.474   196.46873 138.08426]\n",
      "900 cost:  11.2486515 \n",
      " Prediction:\n",
      " [156.54878 181.25961 182.47002 196.46603 138.09575]\n",
      "910 cost:  11.189769 \n",
      " Prediction:\n",
      " [156.53586 181.26842 182.46605 196.46332 138.10718]\n",
      "920 cost:  11.131147 \n",
      " Prediction:\n",
      " [156.523   181.27724 182.46207 196.46063 138.11859]\n",
      "930 cost:  11.07282 \n",
      " Prediction:\n",
      " [156.51015 181.28603 182.4581  196.45795 138.12997]\n",
      "940 cost:  11.014829 \n",
      " Prediction:\n",
      " [156.49734 181.29482 182.45418 196.45529 138.14133]\n",
      "950 cost:  10.957144 \n",
      " Prediction:\n",
      " [156.48457 181.30354 182.45024 196.4526  138.15266]\n",
      "960 cost:  10.89981 \n",
      " Prediction:\n",
      " [156.47182 181.31224 182.4463  196.44995 138.16393]\n",
      "970 cost:  10.842725 \n",
      " Prediction:\n",
      " [156.4591  181.32094 182.44238 196.44728 138.17519]\n",
      "980 cost:  10.785972 \n",
      " Prediction:\n",
      " [156.44644 181.3296  182.43846 196.44464 138.18642]\n",
      "990 cost:  10.729522 \n",
      " Prediction:\n",
      " [156.43379 181.33826 182.43457 196.442   138.1976 ]\n",
      "1000 cost:  10.673368 \n",
      " Prediction:\n",
      " [156.42122 181.34691 182.43071 196.43938 138.2088 ]\n",
      "1010 cost:  10.617575 \n",
      " Prediction:\n",
      " [156.40863 181.35548 182.42683 196.43675 138.21991]\n",
      "1020 cost:  10.56204 \n",
      " Prediction:\n",
      " [156.3961  181.36406 182.42296 196.43413 138.23102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030 cost:  10.506793 \n",
      " Prediction:\n",
      " [156.38359 181.3726  182.4191  196.43153 138.2421 ]\n",
      "1040 cost:  10.4519 \n",
      " Prediction:\n",
      " [156.37112 181.38112 182.41525 196.42892 138.25313]\n",
      "1050 cost:  10.39722 \n",
      " Prediction:\n",
      " [156.35867 181.38963 182.41142 196.42632 138.26414]\n",
      "1060 cost:  10.342865 \n",
      " Prediction:\n",
      " [156.34628 181.39813 182.4076  196.42374 138.27513]\n",
      "1070 cost:  10.2888775 \n",
      " Prediction:\n",
      " [156.33392 181.40659 182.4038  196.42117 138.28607]\n",
      "1080 cost:  10.235085 \n",
      " Prediction:\n",
      " [156.3216  181.41504 182.4     196.41858 138.29701]\n",
      "1090 cost:  10.181647 \n",
      " Prediction:\n",
      " [156.3093  181.42343 182.3962  196.41602 138.3079 ]\n",
      "1100 cost:  10.128407 \n",
      " Prediction:\n",
      " [156.29701 181.43184 182.3924  196.41345 138.31877]\n",
      "1110 cost:  10.075548 \n",
      " Prediction:\n",
      " [156.28477 181.4402  182.38863 196.41089 138.32959]\n",
      "1120 cost:  10.022908 \n",
      " Prediction:\n",
      " [156.27257 181.44856 182.38487 196.40837 138.34041]\n",
      "1130 cost:  9.970617 \n",
      " Prediction:\n",
      " [156.2604  181.45688 182.38112 196.40584 138.35118]\n",
      "1140 cost:  9.918573 \n",
      " Prediction:\n",
      " [156.24826 181.46518 182.37738 196.4033  138.36192]\n",
      "1150 cost:  9.866796 \n",
      " Prediction:\n",
      " [156.23616 181.47346 182.37364 196.40079 138.37265]\n",
      "1160 cost:  9.815354 \n",
      " Prediction:\n",
      " [156.22409 181.4817  182.36992 196.39827 138.38333]\n",
      "1170 cost:  9.76414 \n",
      " Prediction:\n",
      " [156.21205 181.48996 182.36621 196.39575 138.39398]\n",
      "1180 cost:  9.713203 \n",
      " Prediction:\n",
      " [156.20006 181.49818 182.36252 196.39326 138.40463]\n",
      "1190 cost:  9.662561 \n",
      " Prediction:\n",
      " [156.18806 181.50636 182.35881 196.39076 138.4152 ]\n",
      "1200 cost:  9.612226 \n",
      " Prediction:\n",
      " [156.17613 181.51453 182.35515 196.38828 138.42578]\n",
      "1210 cost:  9.562079 \n",
      " Prediction:\n",
      " [156.16422 181.52267 182.35147 196.38579 138.43634]\n",
      "1220 cost:  9.512232 \n",
      " Prediction:\n",
      " [156.15233 181.53079 182.3478  196.38333 138.44685]\n",
      "1230 cost:  9.462667 \n",
      " Prediction:\n",
      " [156.14049 181.53891 182.34415 196.38087 138.45734]\n",
      "1240 cost:  9.413366 \n",
      " Prediction:\n",
      " [156.12866 181.54698 182.3405  196.3784  138.46779]\n",
      "1250 cost:  9.364332 \n",
      " Prediction:\n",
      " [156.11688 181.55505 182.33687 196.37595 138.47821]\n",
      "1260 cost:  9.315565 \n",
      " Prediction:\n",
      " [156.10513 181.5631  182.33325 196.37352 138.48862]\n",
      "1270 cost:  9.267103 \n",
      " Prediction:\n",
      " [156.09341 181.57109 182.32964 196.37108 138.49898]\n",
      "1280 cost:  9.2188425 \n",
      " Prediction:\n",
      " [156.08173 181.57909 182.32602 196.36862 138.50932]\n",
      "1290 cost:  9.17083 \n",
      " Prediction:\n",
      " [156.07008 181.58708 182.32242 196.36623 138.51965]\n",
      "1300 cost:  9.123179 \n",
      " Prediction:\n",
      " [156.05846 181.59502 182.31886 196.3638  138.52992]\n",
      "1310 cost:  9.075682 \n",
      " Prediction:\n",
      " [156.04686 181.60295 182.31528 196.3614  138.54019]\n",
      "1320 cost:  9.02842 \n",
      " Prediction:\n",
      " [156.03528 181.61087 182.31169 196.359   138.55042]\n",
      "1330 cost:  8.981508 \n",
      " Prediction:\n",
      " [156.02374 181.61874 182.30815 196.3566  138.56061]\n",
      "1340 cost:  8.934825 \n",
      " Prediction:\n",
      " [156.01225 181.6266  182.3046  196.35422 138.57079]\n",
      "1350 cost:  8.888393 \n",
      " Prediction:\n",
      " [156.0008  181.63445 182.30106 196.35182 138.58093]\n",
      "1360 cost:  8.842143 \n",
      " Prediction:\n",
      " [155.98933 181.64229 182.29753 196.34944 138.59105]\n",
      "1370 cost:  8.796221 \n",
      " Prediction:\n",
      " [155.97794 181.65007 182.294   196.34708 138.60115]\n",
      "1380 cost:  8.750505 \n",
      " Prediction:\n",
      " [155.96655 181.65787 182.2905  196.34473 138.6112 ]\n",
      "1390 cost:  8.705034 \n",
      " Prediction:\n",
      " [155.9552  181.66563 182.287   196.34236 138.62125]\n",
      "1400 cost:  8.659865 \n",
      " Prediction:\n",
      " [155.9439  181.67337 182.28351 196.34003 138.63124]\n",
      "1410 cost:  8.614889 \n",
      " Prediction:\n",
      " [155.93262 181.68109 182.28004 196.33766 138.64125]\n",
      "1420 cost:  8.570135 \n",
      " Prediction:\n",
      " [155.92134 181.6888  182.27655 196.33533 138.65118]\n",
      "1430 cost:  8.525706 \n",
      " Prediction:\n",
      " [155.91014 181.69647 182.2731  196.33302 138.66112]\n",
      "1440 cost:  8.481461 \n",
      " Prediction:\n",
      " [155.89894 181.70412 182.26964 196.33069 138.67102]\n",
      "1450 cost:  8.437454 \n",
      " Prediction:\n",
      " [155.88777 181.71176 182.2662  196.32838 138.6809 ]\n",
      "1460 cost:  8.393675 \n",
      " Prediction:\n",
      " [155.87663 181.71938 182.26277 196.32608 138.69075]\n",
      "1470 cost:  8.350149 \n",
      " Prediction:\n",
      " [155.86555 181.72699 182.25934 196.32378 138.70058]\n",
      "1480 cost:  8.3068905 \n",
      " Prediction:\n",
      " [155.85446 181.73454 182.25592 196.32147 138.71034]\n",
      "1490 cost:  8.263822 \n",
      " Prediction:\n",
      " [155.84341 181.7421  182.2525  196.31918 138.72011]\n",
      "1500 cost:  8.220985 \n",
      " Prediction:\n",
      " [155.83238 181.74962 182.2491  196.3169  138.72984]\n",
      "1510 cost:  8.178385 \n",
      " Prediction:\n",
      " [155.8214  181.75714 182.24573 196.31462 138.73956]\n",
      "1520 cost:  8.136008 \n",
      " Prediction:\n",
      " [155.81044 181.76465 182.24236 196.31236 138.74925]\n",
      "1530 cost:  8.093912 \n",
      " Prediction:\n",
      " [155.79953 181.77213 182.23898 196.31009 138.7589 ]\n",
      "1540 cost:  8.051956 \n",
      " Prediction:\n",
      " [155.78864 181.77959 182.23561 196.30785 138.76855]\n",
      "1550 cost:  8.010286 \n",
      " Prediction:\n",
      " [155.77776 181.787   182.23225 196.30559 138.77815]\n",
      "1560 cost:  7.9688506 \n",
      " Prediction:\n",
      " [155.7669  181.7944  182.22891 196.30334 138.78772]\n",
      "1570 cost:  7.9276075 \n",
      " Prediction:\n",
      " [155.7561  181.8018  182.22559 196.3011  138.79729]\n",
      "1580 cost:  7.8866286 \n",
      " Prediction:\n",
      " [155.74533 181.80917 182.22224 196.29887 138.8068 ]\n",
      "1590 cost:  7.8458166 \n",
      " Prediction:\n",
      " [155.73457 181.81653 182.21892 196.29666 138.8163 ]\n",
      "1600 cost:  7.8052254 \n",
      " Prediction:\n",
      " [155.72385 181.82387 182.21562 196.29443 138.82579]\n",
      "1610 cost:  7.7648683 \n",
      " Prediction:\n",
      " [155.71315 181.83119 182.21233 196.29222 138.83524]\n",
      "1620 cost:  7.724744 \n",
      " Prediction:\n",
      " [155.70248 181.83849 182.20903 196.29002 138.84465]\n",
      "1630 cost:  7.684839 \n",
      " Prediction:\n",
      " [155.69186 181.84576 182.20578 196.28784 138.85408]\n",
      "1640 cost:  7.6450973 \n",
      " Prediction:\n",
      " [155.68123 181.85301 182.20247 196.28564 138.86343]\n",
      "1650 cost:  7.6056275 \n",
      " Prediction:\n",
      " [155.67065 181.86024 182.19922 196.28346 138.87279]\n",
      "1660 cost:  7.5663505 \n",
      " Prediction:\n",
      " [155.6601  181.86746 182.19595 196.28128 138.8821 ]\n",
      "1670 cost:  7.5272913 \n",
      " Prediction:\n",
      " [155.64958 181.87466 182.19272 196.2791  138.8914 ]\n",
      "1680 cost:  7.4884634 \n",
      " Prediction:\n",
      " [155.6391  181.88184 182.18947 196.27692 138.90067]\n",
      "1690 cost:  7.4498153 \n",
      " Prediction:\n",
      " [155.62862 181.88899 182.18623 196.27477 138.9099 ]\n",
      "1700 cost:  7.411406 \n",
      " Prediction:\n",
      " [155.6182  181.89613 182.18303 196.27263 138.91913]\n",
      "1710 cost:  7.3731475 \n",
      " Prediction:\n",
      " [155.60777 181.90324 182.1798  196.27046 138.92833]\n",
      "1720 cost:  7.335138 \n",
      " Prediction:\n",
      " [155.5974  181.91035 182.1766  196.26833 138.9375 ]\n",
      "1730 cost:  7.2973466 \n",
      " Prediction:\n",
      " [155.58704 181.91742 182.17342 196.26619 138.94664]\n",
      "1740 cost:  7.259739 \n",
      " Prediction:\n",
      " [155.5767  181.92448 182.17023 196.26405 138.95575]\n",
      "1750 cost:  7.222285 \n",
      " Prediction:\n",
      " [155.5664  181.93153 182.16702 196.26195 138.96486]\n",
      "1760 cost:  7.185067 \n",
      " Prediction:\n",
      " [155.55614 181.93857 182.16386 196.25981 138.97394]\n",
      "1770 cost:  7.1480775 \n",
      " Prediction:\n",
      " [155.5459  181.94557 182.16069 196.2577  138.98297]\n",
      "1780 cost:  7.111305 \n",
      " Prediction:\n",
      " [155.53566 181.95253 182.15755 196.2556  138.99197]\n",
      "1790 cost:  7.074678 \n",
      " Prediction:\n",
      " [155.5255  181.95953 182.15443 196.25352 139.001  ]\n",
      "1800 cost:  7.0382676 \n",
      " Prediction:\n",
      " [155.51532 181.96648 182.15128 196.2514  139.00995]\n",
      "1810 cost:  7.0020576 \n",
      " Prediction:\n",
      " [155.5052  181.9734  182.14813 196.24931 139.0189 ]\n",
      "1820 cost:  6.9660277 \n",
      " Prediction:\n",
      " [155.49509 181.98032 182.14502 196.24724 139.02783]\n",
      "1830 cost:  6.930246 \n",
      " Prediction:\n",
      " [155.48502 181.9872  182.14192 196.24516 139.03673]\n",
      "1840 cost:  6.8946047 \n",
      " Prediction:\n",
      " [155.47496 181.99408 182.13881 196.24309 139.0456 ]\n",
      "1850 cost:  6.859151 \n",
      " Prediction:\n",
      " [155.46492 182.00092 182.13571 196.24103 139.05446]\n",
      "1860 cost:  6.8238907 \n",
      " Prediction:\n",
      " [155.45493 182.00777 182.13261 196.23897 139.06328]\n",
      "1870 cost:  6.788846 \n",
      " Prediction:\n",
      " [155.44495 182.01457 182.12955 196.23691 139.07208]\n",
      "1880 cost:  6.7540016 \n",
      " Prediction:\n",
      " [155.43501 182.02138 182.12646 196.23486 139.08084]\n",
      "1890 cost:  6.7193136 \n",
      " Prediction:\n",
      " [155.42508 182.02817 182.12341 196.23283 139.08958]\n",
      "1900 cost:  6.6848006 \n",
      " Prediction:\n",
      " [155.41519 182.03493 182.12036 196.2308  139.09834]\n",
      "1910 cost:  6.6505127 \n",
      " Prediction:\n",
      " [155.40533 182.04167 182.11731 196.22878 139.10704]\n",
      "1920 cost:  6.616385 \n",
      " Prediction:\n",
      " [155.3955  182.0484  182.11427 196.22675 139.11572]\n",
      "1930 cost:  6.582442 \n",
      " Prediction:\n",
      " [155.38568 182.05511 182.11125 196.22473 139.12439]\n",
      "1940 cost:  6.5486703 \n",
      " Prediction:\n",
      " [155.37589 182.06181 182.10822 196.22273 139.13301]\n",
      "1950 cost:  6.5151243 \n",
      " Prediction:\n",
      " [155.36613 182.06848 182.10522 196.22073 139.14163]\n",
      "1960 cost:  6.4817343 \n",
      " Prediction:\n",
      " [155.3564  182.07513 182.1022  196.21872 139.1502 ]\n",
      "1970 cost:  6.448504 \n",
      " Prediction:\n",
      " [155.34668 182.08177 182.0992  196.21672 139.15877]\n",
      "1980 cost:  6.4154863 \n",
      " Prediction:\n",
      " [155.337   182.0884  182.09622 196.21474 139.16731]\n",
      "1990 cost:  6.3826513 \n",
      " Prediction:\n",
      " [155.32736 182.09499 182.09325 196.21275 139.17584]\n",
      "2000 cost:  6.349966 \n",
      " Prediction:\n",
      " [155.31773 182.10156 182.09026 196.21078 139.18434]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1_data = [73., 93., 89., 96., 73.]\n",
    "x2_data = [80., 88., 91., 98., 66.]\n",
    "x3_data = [75., 93., 90., 100., 70.]\n",
    "y_data = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# Placeholders for a tensor that will be always fed.\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]), name = 'weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name = 'weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name = 'weight3')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 +b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                  feed_dict = {x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
    "    if step % 10 == 0:\n",
    "        print(step, \"cost: \", cost_val, \"\\n Prediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에처럼 하면 data가 100개일때 다 일일이 코드를 칠 수 없음 그래서 이때 matrix(행렬)을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  79349.06 \n",
      "Prediction:\n",
      " [[ -99.58818]\n",
      " [-115.0349 ]\n",
      " [-115.68413]\n",
      " [-126.0509 ]\n",
      " [ -86.65231]]\n",
      "10 Cost:  1.3783334 \n",
      "Prediction:\n",
      " [[149.6006 ]\n",
      " [184.46672]\n",
      " [179.42241]\n",
      " [195.31229]\n",
      " [141.79118]]\n",
      "20 Cost:  0.6497844 \n",
      "Prediction:\n",
      " [[150.35706]\n",
      " [185.37125]\n",
      " [180.31609]\n",
      " [196.28506]\n",
      " [142.48027]]\n",
      "30 Cost:  0.64763695 \n",
      "Prediction:\n",
      " [[150.36182]\n",
      " [185.37228]\n",
      " [180.31955]\n",
      " [196.28842]\n",
      " [142.48026]]\n",
      "40 Cost:  0.64550805 \n",
      "Prediction:\n",
      " [[150.3643 ]\n",
      " [185.3706 ]\n",
      " [180.32034]\n",
      " [196.28886]\n",
      " [142.47815]]\n",
      "50 Cost:  0.6433898 \n",
      "Prediction:\n",
      " [[150.36678]\n",
      " [185.36891]\n",
      " [180.3211 ]\n",
      " [196.28929]\n",
      " [142.47606]]\n",
      "60 Cost:  0.641288 \n",
      "Prediction:\n",
      " [[150.36925]\n",
      " [185.36723]\n",
      " [180.3219 ]\n",
      " [196.28973]\n",
      " [142.47398]]\n",
      "70 Cost:  0.6391976 \n",
      "Prediction:\n",
      " [[150.3717 ]\n",
      " [185.36559]\n",
      " [180.32266]\n",
      " [196.29016]\n",
      " [142.4719 ]]\n",
      "80 Cost:  0.6371194 \n",
      "Prediction:\n",
      " [[150.37415]\n",
      " [185.3639 ]\n",
      " [180.32343]\n",
      " [196.29057]\n",
      " [142.46982]]\n",
      "90 Cost:  0.6350526 \n",
      "Prediction:\n",
      " [[150.37659]\n",
      " [185.36224]\n",
      " [180.3242 ]\n",
      " [196.291  ]\n",
      " [142.46774]]\n",
      "100 Cost:  0.6329916 \n",
      "Prediction:\n",
      " [[150.37901]\n",
      " [185.36057]\n",
      " [180.32495]\n",
      " [196.2914 ]\n",
      " [142.46567]]\n",
      "110 Cost:  0.6309439 \n",
      "Prediction:\n",
      " [[150.38145]\n",
      " [185.35893]\n",
      " [180.32571]\n",
      " [196.29182]\n",
      " [142.46362]]\n",
      "120 Cost:  0.6288985 \n",
      "Prediction:\n",
      " [[150.3839 ]\n",
      " [185.35728]\n",
      " [180.32648]\n",
      " [196.29227]\n",
      " [142.46156]]\n",
      "130 Cost:  0.62687564 \n",
      "Prediction:\n",
      " [[150.3863 ]\n",
      " [185.35564]\n",
      " [180.32724]\n",
      " [196.29266]\n",
      " [142.45952]]\n",
      "140 Cost:  0.624864 \n",
      "Prediction:\n",
      " [[150.38872]\n",
      " [185.354  ]\n",
      " [180.328  ]\n",
      " [196.29309]\n",
      " [142.45747]]\n",
      "150 Cost:  0.62285256 \n",
      "Prediction:\n",
      " [[150.39113]\n",
      " [185.35236]\n",
      " [180.32874]\n",
      " [196.2935 ]\n",
      " [142.45544]]\n",
      "160 Cost:  0.6208595 \n",
      "Prediction:\n",
      " [[150.39352]\n",
      " [185.35072]\n",
      " [180.3295 ]\n",
      " [196.29391]\n",
      " [142.4534 ]]\n",
      "170 Cost:  0.61887753 \n",
      "Prediction:\n",
      " [[150.39592]\n",
      " [185.3491 ]\n",
      " [180.33025]\n",
      " [196.29433]\n",
      " [142.45139]]\n",
      "180 Cost:  0.6168998 \n",
      "Prediction:\n",
      " [[150.3983 ]\n",
      " [185.34746]\n",
      " [180.33098]\n",
      " [196.29472]\n",
      " [142.44936]]\n",
      "190 Cost:  0.61494106 \n",
      "Prediction:\n",
      " [[150.4007 ]\n",
      " [185.34587]\n",
      " [180.33176]\n",
      " [196.29515]\n",
      " [142.44736]]\n",
      "200 Cost:  0.6129905 \n",
      "Prediction:\n",
      " [[150.40306]\n",
      " [185.34425]\n",
      " [180.33249]\n",
      " [196.29555]\n",
      " [142.44534]]\n",
      "210 Cost:  0.6110486 \n",
      "Prediction:\n",
      " [[150.40544]\n",
      " [185.34265]\n",
      " [180.33324]\n",
      " [196.29597]\n",
      " [142.44336]]\n",
      "220 Cost:  0.60911804 \n",
      "Prediction:\n",
      " [[150.40779]\n",
      " [185.34103]\n",
      " [180.33397]\n",
      " [196.29637]\n",
      " [142.44135]]\n",
      "230 Cost:  0.6071974 \n",
      "Prediction:\n",
      " [[150.41014]\n",
      " [185.33943]\n",
      " [180.3347 ]\n",
      " [196.29677]\n",
      " [142.43935]]\n",
      "240 Cost:  0.6052815 \n",
      "Prediction:\n",
      " [[150.4125 ]\n",
      " [185.33783]\n",
      " [180.33545]\n",
      " [196.29718]\n",
      " [142.43738]]\n",
      "250 Cost:  0.60338837 \n",
      "Prediction:\n",
      " [[150.41483]\n",
      " [185.33623]\n",
      " [180.33617]\n",
      " [196.29758]\n",
      " [142.43538]]\n",
      "260 Cost:  0.60149646 \n",
      "Prediction:\n",
      " [[150.41716]\n",
      " [185.33464]\n",
      " [180.3369 ]\n",
      " [196.29794]\n",
      " [142.43341]]\n",
      "270 Cost:  0.59961385 \n",
      "Prediction:\n",
      " [[150.41951]\n",
      " [185.33307]\n",
      " [180.33765]\n",
      " [196.29837]\n",
      " [142.43146]]\n",
      "280 Cost:  0.5977367 \n",
      "Prediction:\n",
      " [[150.42183]\n",
      " [185.33147]\n",
      " [180.33836]\n",
      " [196.29875]\n",
      " [142.42947]]\n",
      "290 Cost:  0.59587973 \n",
      "Prediction:\n",
      " [[150.42415]\n",
      " [185.32991]\n",
      " [180.3391 ]\n",
      " [196.29915]\n",
      " [142.42752]]\n",
      "300 Cost:  0.59403384 \n",
      "Prediction:\n",
      " [[150.42647]\n",
      " [185.32835]\n",
      " [180.33983]\n",
      " [196.29958]\n",
      " [142.42558]]\n",
      "310 Cost:  0.5921942 \n",
      "Prediction:\n",
      " [[150.42876]\n",
      " [185.32675]\n",
      " [180.34055]\n",
      " [196.29994]\n",
      " [142.42363]]\n",
      "320 Cost:  0.59035844 \n",
      "Prediction:\n",
      " [[150.43108]\n",
      " [185.32523]\n",
      " [180.34128]\n",
      " [196.30034]\n",
      " [142.42169]]\n",
      "330 Cost:  0.5885328 \n",
      "Prediction:\n",
      " [[150.43336]\n",
      " [185.32364]\n",
      " [180.34198]\n",
      " [196.30072]\n",
      " [142.41975]]\n",
      "340 Cost:  0.5867265 \n",
      "Prediction:\n",
      " [[150.43565]\n",
      " [185.3221 ]\n",
      " [180.34271]\n",
      " [196.30112]\n",
      " [142.41783]]\n",
      "350 Cost:  0.5849175 \n",
      "Prediction:\n",
      " [[150.43793]\n",
      " [185.32053]\n",
      " [180.3434 ]\n",
      " [196.3015 ]\n",
      " [142.41588]]\n",
      "360 Cost:  0.5831274 \n",
      "Prediction:\n",
      " [[150.44022]\n",
      " [185.319  ]\n",
      " [180.34415]\n",
      " [196.3019 ]\n",
      " [142.41397]]\n",
      "370 Cost:  0.58134896 \n",
      "Prediction:\n",
      " [[150.44247]\n",
      " [185.31744]\n",
      " [180.34485]\n",
      " [196.30228]\n",
      " [142.41206]]\n",
      "380 Cost:  0.579573 \n",
      "Prediction:\n",
      " [[150.44473]\n",
      " [185.3159 ]\n",
      " [180.34555]\n",
      " [196.30264]\n",
      " [142.41014]]\n",
      "390 Cost:  0.5778048 \n",
      "Prediction:\n",
      " [[150.447  ]\n",
      " [185.31438]\n",
      " [180.34627]\n",
      " [196.30304]\n",
      " [142.40825]]\n",
      "400 Cost:  0.57605386 \n",
      "Prediction:\n",
      " [[150.44925]\n",
      " [185.31285]\n",
      " [180.34697]\n",
      " [196.30342]\n",
      " [142.40634]]\n",
      "410 Cost:  0.57430273 \n",
      "Prediction:\n",
      " [[150.4515 ]\n",
      " [185.31133]\n",
      " [180.34769]\n",
      " [196.3038 ]\n",
      " [142.40445]]\n",
      "420 Cost:  0.5725671 \n",
      "Prediction:\n",
      " [[150.45374]\n",
      " [185.30978]\n",
      " [180.34837]\n",
      " [196.30417]\n",
      " [142.40256]]\n",
      "430 Cost:  0.57084334 \n",
      "Prediction:\n",
      " [[150.45598]\n",
      " [185.30827]\n",
      " [180.34909]\n",
      " [196.30457]\n",
      " [142.4007 ]]\n",
      "440 Cost:  0.56911695 \n",
      "Prediction:\n",
      " [[150.4582 ]\n",
      " [185.30675]\n",
      " [180.34978]\n",
      " [196.30492]\n",
      " [142.3988 ]]\n",
      "450 Cost:  0.56741256 \n",
      "Prediction:\n",
      " [[150.46042]\n",
      " [185.30524]\n",
      " [180.35046]\n",
      " [196.3053 ]\n",
      " [142.39693]]\n",
      "460 Cost:  0.5657116 \n",
      "Prediction:\n",
      " [[150.46265]\n",
      " [185.30374]\n",
      " [180.35117]\n",
      " [196.3057 ]\n",
      " [142.39507]]\n",
      "470 Cost:  0.56402427 \n",
      "Prediction:\n",
      " [[150.46484]\n",
      " [185.30223]\n",
      " [180.35185]\n",
      " [196.30605]\n",
      " [142.3932 ]]\n",
      "480 Cost:  0.5623397 \n",
      "Prediction:\n",
      " [[150.46706]\n",
      " [185.30075]\n",
      " [180.35255]\n",
      " [196.30641]\n",
      " [142.39134]]\n",
      "490 Cost:  0.5606657 \n",
      "Prediction:\n",
      " [[150.46925]\n",
      " [185.29922]\n",
      " [180.35324]\n",
      " [196.3068 ]\n",
      " [142.3895 ]]\n",
      "500 Cost:  0.55900556 \n",
      "Prediction:\n",
      " [[150.47145]\n",
      " [185.29776]\n",
      " [180.35394]\n",
      " [196.30717]\n",
      " [142.38765]]\n",
      "510 Cost:  0.5573444 \n",
      "Prediction:\n",
      " [[150.47365]\n",
      " [185.29628]\n",
      " [180.35463]\n",
      " [196.30753]\n",
      " [142.38582]]\n",
      "520 Cost:  0.55570185 \n",
      "Prediction:\n",
      " [[150.47581]\n",
      " [185.29478]\n",
      " [180.3553 ]\n",
      " [196.30789]\n",
      " [142.38397]]\n",
      "530 Cost:  0.5540565 \n",
      "Prediction:\n",
      " [[150.47803]\n",
      " [185.29333]\n",
      " [180.35602]\n",
      " [196.30827]\n",
      " [142.38217]]\n",
      "540 Cost:  0.5524272 \n",
      "Prediction:\n",
      " [[150.48016]\n",
      " [185.29181]\n",
      " [180.35666]\n",
      " [196.30861]\n",
      " [142.38031]]\n",
      "550 Cost:  0.550814 \n",
      "Prediction:\n",
      " [[150.48233]\n",
      " [185.29036]\n",
      " [180.35736]\n",
      " [196.30899]\n",
      " [142.3785 ]]\n",
      "560 Cost:  0.54920256 \n",
      "Prediction:\n",
      " [[150.48448]\n",
      " [185.28888]\n",
      " [180.35803]\n",
      " [196.30934]\n",
      " [142.37668]]\n",
      "570 Cost:  0.54759383 \n",
      "Prediction:\n",
      " [[150.48665]\n",
      " [185.28743]\n",
      " [180.3587 ]\n",
      " [196.30971]\n",
      " [142.37488]]\n",
      "580 Cost:  0.5460042 \n",
      "Prediction:\n",
      " [[150.48878]\n",
      " [185.28596]\n",
      " [180.35938]\n",
      " [196.31006]\n",
      " [142.37308]]\n",
      "590 Cost:  0.5444205 \n",
      "Prediction:\n",
      " [[150.49092]\n",
      " [185.28452]\n",
      " [180.36005]\n",
      " [196.31041]\n",
      " [142.37128]]\n",
      "600 Cost:  0.54284286 \n",
      "Prediction:\n",
      " [[150.49306]\n",
      " [185.28307]\n",
      " [180.36072]\n",
      " [196.31078]\n",
      " [142.36948]]\n",
      "610 Cost:  0.54126984 \n",
      "Prediction:\n",
      " [[150.4952 ]\n",
      " [185.28162]\n",
      " [180.36139]\n",
      " [196.31113]\n",
      " [142.36769]]\n",
      "620 Cost:  0.5397026 \n",
      "Prediction:\n",
      " [[150.49733]\n",
      " [185.28018]\n",
      " [180.36206]\n",
      " [196.31148]\n",
      " [142.3659 ]]\n",
      "630 Cost:  0.5381578 \n",
      "Prediction:\n",
      " [[150.49944]\n",
      " [185.27875]\n",
      " [180.36273]\n",
      " [196.31183]\n",
      " [142.36412]]\n",
      "640 Cost:  0.53660834 \n",
      "Prediction:\n",
      " [[150.50156]\n",
      " [185.2773 ]\n",
      " [180.36339]\n",
      " [196.3122 ]\n",
      " [142.36235]]\n",
      "650 Cost:  0.5350699 \n",
      "Prediction:\n",
      " [[150.50366]\n",
      " [185.27586]\n",
      " [180.36404]\n",
      " [196.31253]\n",
      " [142.36058]]\n",
      "660 Cost:  0.5335455 \n",
      "Prediction:\n",
      " [[150.50577]\n",
      " [185.27444]\n",
      " [180.36472]\n",
      " [196.3129 ]\n",
      " [142.35883]]\n",
      "670 Cost:  0.53201985 \n",
      "Prediction:\n",
      " [[150.50786]\n",
      " [185.27301]\n",
      " [180.36536]\n",
      " [196.31323]\n",
      " [142.35704]]\n",
      "680 Cost:  0.5305115 \n",
      "Prediction:\n",
      " [[150.50996]\n",
      " [185.27162]\n",
      " [180.36604]\n",
      " [196.31358]\n",
      " [142.35532]]\n",
      "690 Cost:  0.52900285 \n",
      "Prediction:\n",
      " [[150.51204]\n",
      " [185.27019]\n",
      " [180.36667]\n",
      " [196.31392]\n",
      " [142.35355]]\n",
      "700 Cost:  0.5275148 \n",
      "Prediction:\n",
      " [[150.51411]\n",
      " [185.26877]\n",
      " [180.36734]\n",
      " [196.31429]\n",
      " [142.3518 ]]\n",
      "710 Cost:  0.52601683 \n",
      "Prediction:\n",
      " [[150.51619]\n",
      " [185.26733]\n",
      " [180.36798]\n",
      " [196.3146 ]\n",
      " [142.35005]]\n",
      "720 Cost:  0.52453816 \n",
      "Prediction:\n",
      " [[150.51826]\n",
      " [185.26595]\n",
      " [180.36865]\n",
      " [196.31494]\n",
      " [142.34833]]\n",
      "730 Cost:  0.523061 \n",
      "Prediction:\n",
      " [[150.52034]\n",
      " [185.26456]\n",
      " [180.3693 ]\n",
      " [196.31529]\n",
      " [142.3466 ]]\n",
      "740 Cost:  0.5215991 \n",
      "Prediction:\n",
      " [[150.5224 ]\n",
      " [185.26317]\n",
      " [180.36995]\n",
      " [196.31563]\n",
      " [142.3449 ]]\n",
      "750 Cost:  0.520146 \n",
      "Prediction:\n",
      " [[150.52444]\n",
      " [185.26176]\n",
      " [180.37059]\n",
      " [196.31598]\n",
      " [142.34317]]\n",
      "760 Cost:  0.5186965 \n",
      "Prediction:\n",
      " [[150.52649]\n",
      " [185.26038]\n",
      " [180.37123]\n",
      " [196.31631]\n",
      " [142.34145]]\n",
      "770 Cost:  0.51724887 \n",
      "Prediction:\n",
      " [[150.52853]\n",
      " [185.25899]\n",
      " [180.37187]\n",
      " [196.31664]\n",
      " [142.33972]]\n",
      "780 Cost:  0.51581645 \n",
      "Prediction:\n",
      " [[150.53056]\n",
      " [185.25758]\n",
      " [180.37251]\n",
      " [196.31697]\n",
      " [142.33801]]\n",
      "790 Cost:  0.51438844 \n",
      "Prediction:\n",
      " [[150.53261]\n",
      " [185.25621]\n",
      " [180.37317]\n",
      " [196.3173 ]\n",
      " [142.33633]]\n",
      "800 Cost:  0.51297146 \n",
      "Prediction:\n",
      " [[150.53462]\n",
      " [185.25484]\n",
      " [180.3738 ]\n",
      " [196.31763]\n",
      " [142.33463]]\n",
      "810 Cost:  0.5115622 \n",
      "Prediction:\n",
      " [[150.53664]\n",
      " [185.25346]\n",
      " [180.37442]\n",
      " [196.31796]\n",
      " [142.33293]]\n",
      "820 Cost:  0.51014984 \n",
      "Prediction:\n",
      " [[150.53867]\n",
      " [185.25209]\n",
      " [180.37505]\n",
      " [196.3183 ]\n",
      " [142.33125]]\n",
      "830 Cost:  0.50875413 \n",
      "Prediction:\n",
      " [[150.54068]\n",
      " [185.25073]\n",
      " [180.3757 ]\n",
      " [196.31863]\n",
      " [142.32956]]\n",
      "840 Cost:  0.50736654 \n",
      "Prediction:\n",
      " [[150.54268]\n",
      " [185.24937]\n",
      " [180.37633]\n",
      " [196.31895]\n",
      " [142.32788]]\n",
      "850 Cost:  0.5059835 \n",
      "Prediction:\n",
      " [[150.5447 ]\n",
      " [185.24803]\n",
      " [180.37697]\n",
      " [196.31929]\n",
      " [142.32623]]\n",
      "860 Cost:  0.5046063 \n",
      "Prediction:\n",
      " [[150.54668]\n",
      " [185.24666]\n",
      " [180.37758]\n",
      " [196.31961]\n",
      " [142.32454]]\n",
      "870 Cost:  0.503244 \n",
      "Prediction:\n",
      " [[150.54866]\n",
      " [185.24532]\n",
      " [180.3782 ]\n",
      " [196.31993]\n",
      " [142.32289]]\n",
      "880 Cost:  0.5018818 \n",
      "Prediction:\n",
      " [[150.55064]\n",
      " [185.24396]\n",
      " [180.37883]\n",
      " [196.32025]\n",
      " [142.32123]]\n",
      "890 Cost:  0.5005266 \n",
      "Prediction:\n",
      " [[150.55263]\n",
      " [185.24261]\n",
      " [180.37946]\n",
      " [196.32057]\n",
      " [142.31958]]\n",
      "900 Cost:  0.4991762 \n",
      "Prediction:\n",
      " [[150.55461]\n",
      " [185.24129]\n",
      " [180.38007]\n",
      " [196.3209 ]\n",
      " [142.31793]]\n",
      "910 Cost:  0.49784192 \n",
      "Prediction:\n",
      " [[150.55656]\n",
      " [185.23994]\n",
      " [180.38069]\n",
      " [196.32121]\n",
      " [142.31627]]\n",
      "920 Cost:  0.49650392 \n",
      "Prediction:\n",
      " [[150.55853]\n",
      " [185.2386 ]\n",
      " [180.3813 ]\n",
      " [196.32152]\n",
      " [142.31464]]\n",
      "930 Cost:  0.495184 \n",
      "Prediction:\n",
      " [[150.56049]\n",
      " [185.23727]\n",
      " [180.38193]\n",
      " [196.32185]\n",
      " [142.31299]]\n",
      "940 Cost:  0.4938666 \n",
      "Prediction:\n",
      " [[150.56244]\n",
      " [185.23596]\n",
      " [180.38254]\n",
      " [196.32216]\n",
      " [142.31137]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 Cost:  0.4925533 \n",
      "Prediction:\n",
      " [[150.56439]\n",
      " [185.23463]\n",
      " [180.38315]\n",
      " [196.3225 ]\n",
      " [142.30974]]\n",
      "960 Cost:  0.4912451 \n",
      "Prediction:\n",
      " [[150.56635]\n",
      " [185.23332]\n",
      " [180.38377]\n",
      " [196.32278]\n",
      " [142.30814]]\n",
      "970 Cost:  0.4899567 \n",
      "Prediction:\n",
      " [[150.56825]\n",
      " [185.232  ]\n",
      " [180.38437]\n",
      " [196.32309]\n",
      " [142.30649]]\n",
      "980 Cost:  0.4886631 \n",
      "Prediction:\n",
      " [[150.5702 ]\n",
      " [185.23068]\n",
      " [180.385  ]\n",
      " [196.32343]\n",
      " [142.3049 ]]\n",
      "990 Cost:  0.48737937 \n",
      "Prediction:\n",
      " [[150.57213]\n",
      " [185.22939]\n",
      " [180.38559]\n",
      " [196.32373]\n",
      " [142.30328]]\n",
      "1000 Cost:  0.48609838 \n",
      "Prediction:\n",
      " [[150.57407]\n",
      " [185.22809]\n",
      " [180.3862 ]\n",
      " [196.32405]\n",
      " [142.3017 ]]\n",
      "1010 Cost:  0.48483604 \n",
      "Prediction:\n",
      " [[150.57596]\n",
      " [185.22676]\n",
      " [180.3868 ]\n",
      " [196.32436]\n",
      " [142.30008]]\n",
      "1020 Cost:  0.48356986 \n",
      "Prediction:\n",
      " [[150.57788]\n",
      " [185.22548]\n",
      " [180.3874 ]\n",
      " [196.32466]\n",
      " [142.2985 ]]\n",
      "1030 Cost:  0.48231015 \n",
      "Prediction:\n",
      " [[150.57979]\n",
      " [185.22418]\n",
      " [180.388  ]\n",
      " [196.32497]\n",
      " [142.29689]]\n",
      "1040 Cost:  0.48106393 \n",
      "Prediction:\n",
      " [[150.58168]\n",
      " [185.22289]\n",
      " [180.3886 ]\n",
      " [196.32526]\n",
      " [142.29532]]\n",
      "1050 Cost:  0.479814 \n",
      "Prediction:\n",
      " [[150.58359]\n",
      " [185.2216 ]\n",
      " [180.38919]\n",
      " [196.32556]\n",
      " [142.29373]]\n",
      "1060 Cost:  0.47858208 \n",
      "Prediction:\n",
      " [[150.58548]\n",
      " [185.22032]\n",
      " [180.3898 ]\n",
      " [196.32588]\n",
      " [142.29216]]\n",
      "1070 Cost:  0.4773503 \n",
      "Prediction:\n",
      " [[150.58736]\n",
      " [185.21902]\n",
      " [180.39038]\n",
      " [196.32617]\n",
      " [142.29057]]\n",
      "1080 Cost:  0.47612363 \n",
      "Prediction:\n",
      " [[150.58925]\n",
      " [185.21776]\n",
      " [180.39098]\n",
      " [196.32648]\n",
      " [142.28902]]\n",
      "1090 Cost:  0.47490177 \n",
      "Prediction:\n",
      " [[150.59113]\n",
      " [185.21648]\n",
      " [180.39157]\n",
      " [196.32675]\n",
      " [142.28745]]\n",
      "1100 Cost:  0.4736939 \n",
      "Prediction:\n",
      " [[150.593  ]\n",
      " [185.21521]\n",
      " [180.39217]\n",
      " [196.32707]\n",
      " [142.2859 ]]\n",
      "1110 Cost:  0.47249407 \n",
      "Prediction:\n",
      " [[150.59485]\n",
      " [185.21394]\n",
      " [180.39273]\n",
      " [196.32736]\n",
      " [142.28433]]\n",
      "1120 Cost:  0.47129226 \n",
      "Prediction:\n",
      " [[150.59673]\n",
      " [185.21269]\n",
      " [180.39334]\n",
      " [196.32765]\n",
      " [142.28279]]\n",
      "1130 Cost:  0.4701066 \n",
      "Prediction:\n",
      " [[150.59857]\n",
      " [185.21143]\n",
      " [180.39392]\n",
      " [196.32796]\n",
      " [142.28125]]\n",
      "1140 Cost:  0.4689107 \n",
      "Prediction:\n",
      " [[150.60045]\n",
      " [185.21017]\n",
      " [180.39452]\n",
      " [196.32826]\n",
      " [142.27971]]\n",
      "1150 Cost:  0.46773046 \n",
      "Prediction:\n",
      " [[150.60228]\n",
      " [185.2089 ]\n",
      " [180.39508]\n",
      " [196.32854]\n",
      " [142.27815]]\n",
      "1160 Cost:  0.46656138 \n",
      "Prediction:\n",
      " [[150.60414]\n",
      " [185.20767]\n",
      " [180.39569]\n",
      " [196.32886]\n",
      " [142.27666]]\n",
      "1170 Cost:  0.46539158 \n",
      "Prediction:\n",
      " [[150.60597]\n",
      " [185.20642]\n",
      " [180.39626]\n",
      " [196.32913]\n",
      " [142.27512]]\n",
      "1180 Cost:  0.4642237 \n",
      "Prediction:\n",
      " [[150.60782]\n",
      " [185.20518]\n",
      " [180.39684]\n",
      " [196.32942]\n",
      " [142.27359]]\n",
      "1190 Cost:  0.46307224 \n",
      "Prediction:\n",
      " [[150.60963]\n",
      " [185.20393]\n",
      " [180.3974 ]\n",
      " [196.32971]\n",
      " [142.27206]]\n",
      "1200 Cost:  0.4619279 \n",
      "Prediction:\n",
      " [[150.61147]\n",
      " [185.20271]\n",
      " [180.398  ]\n",
      " [196.33002]\n",
      " [142.27057]]\n",
      "1210 Cost:  0.46078047 \n",
      "Prediction:\n",
      " [[150.61328]\n",
      " [185.20146]\n",
      " [180.39856]\n",
      " [196.33029]\n",
      " [142.26904]]\n",
      "1220 Cost:  0.4596409 \n",
      "Prediction:\n",
      " [[150.6151 ]\n",
      " [185.20023]\n",
      " [180.39912]\n",
      " [196.33058]\n",
      " [142.26753]]\n",
      "1230 Cost:  0.4585144 \n",
      "Prediction:\n",
      " [[150.6169 ]\n",
      " [185.199  ]\n",
      " [180.3997 ]\n",
      " [196.33086]\n",
      " [142.26602]]\n",
      "1240 Cost:  0.4573837 \n",
      "Prediction:\n",
      " [[150.61871]\n",
      " [185.19778]\n",
      " [180.40027]\n",
      " [196.33115]\n",
      " [142.26453]]\n",
      "1250 Cost:  0.4562663 \n",
      "Prediction:\n",
      " [[150.62051]\n",
      " [185.19656]\n",
      " [180.40083]\n",
      " [196.33145]\n",
      " [142.26303]]\n",
      "1260 Cost:  0.4551559 \n",
      "Prediction:\n",
      " [[150.6223 ]\n",
      " [185.19534]\n",
      " [180.4014 ]\n",
      " [196.33171]\n",
      " [142.26155]]\n",
      "1270 Cost:  0.4540488 \n",
      "Prediction:\n",
      " [[150.62408]\n",
      " [185.19412]\n",
      " [180.40196]\n",
      " [196.33199]\n",
      " [142.26006]]\n",
      "1280 Cost:  0.45294505 \n",
      "Prediction:\n",
      " [[150.62589]\n",
      " [185.19292]\n",
      " [180.40254]\n",
      " [196.33228]\n",
      " [142.25859]]\n",
      "1290 Cost:  0.4518529 \n",
      "Prediction:\n",
      " [[150.62766]\n",
      " [185.19171]\n",
      " [180.40309]\n",
      " [196.33257]\n",
      " [142.2571 ]]\n",
      "1300 Cost:  0.4507563 \n",
      "Prediction:\n",
      " [[150.62944]\n",
      " [185.1905 ]\n",
      " [180.40364]\n",
      " [196.33284]\n",
      " [142.25563]]\n",
      "1310 Cost:  0.44967908 \n",
      "Prediction:\n",
      " [[150.6312 ]\n",
      " [185.18929]\n",
      " [180.4042 ]\n",
      " [196.33311]\n",
      " [142.25415]]\n",
      "1320 Cost:  0.44860297 \n",
      "Prediction:\n",
      " [[150.63297]\n",
      " [185.18811]\n",
      " [180.40477]\n",
      " [196.33339]\n",
      " [142.2527 ]]\n",
      "1330 Cost:  0.44752604 \n",
      "Prediction:\n",
      " [[150.63472]\n",
      " [185.1869 ]\n",
      " [180.40532]\n",
      " [196.33363]\n",
      " [142.25122]]\n",
      "1340 Cost:  0.4464539 \n",
      "Prediction:\n",
      " [[150.63649]\n",
      " [185.18571]\n",
      " [180.40587]\n",
      " [196.33392]\n",
      " [142.24977]]\n",
      "1350 Cost:  0.44538775 \n",
      "Prediction:\n",
      " [[150.63824]\n",
      " [185.18451]\n",
      " [180.40642]\n",
      " [196.33418]\n",
      " [142.24832]]\n",
      "1360 Cost:  0.44433576 \n",
      "Prediction:\n",
      " [[150.64   ]\n",
      " [185.18335]\n",
      " [180.40698]\n",
      " [196.33447]\n",
      " [142.24689]]\n",
      "1370 Cost:  0.44329482 \n",
      "Prediction:\n",
      " [[150.64172]\n",
      " [185.18216]\n",
      " [180.40753]\n",
      " [196.33475]\n",
      " [142.24544]]\n",
      "1380 Cost:  0.4422396 \n",
      "Prediction:\n",
      " [[150.64348]\n",
      " [185.18097]\n",
      " [180.40808]\n",
      " [196.335  ]\n",
      " [142.244  ]]\n",
      "1390 Cost:  0.44120535 \n",
      "Prediction:\n",
      " [[150.6452 ]\n",
      " [185.1798 ]\n",
      " [180.40863]\n",
      " [196.33528]\n",
      " [142.24255]]\n",
      "1400 Cost:  0.44016248 \n",
      "Prediction:\n",
      " [[150.64694]\n",
      " [185.17862]\n",
      " [180.40916]\n",
      " [196.33554]\n",
      " [142.24112]]\n",
      "1410 Cost:  0.43914014 \n",
      "Prediction:\n",
      " [[150.64867]\n",
      " [185.17746]\n",
      " [180.40973]\n",
      " [196.33582]\n",
      " [142.2397 ]]\n",
      "1420 Cost:  0.43812123 \n",
      "Prediction:\n",
      " [[150.65038]\n",
      " [185.17628]\n",
      " [180.41026]\n",
      " [196.33609]\n",
      " [142.23827]]\n",
      "1430 Cost:  0.4371012 \n",
      "Prediction:\n",
      " [[150.6521 ]\n",
      " [185.17513]\n",
      " [180.41081]\n",
      " [196.33635]\n",
      " [142.23686]]\n",
      "1440 Cost:  0.43608698 \n",
      "Prediction:\n",
      " [[150.65381]\n",
      " [185.17395]\n",
      " [180.41135]\n",
      " [196.33661]\n",
      " [142.23544]]\n",
      "1450 Cost:  0.43506822 \n",
      "Prediction:\n",
      " [[150.65555]\n",
      " [185.17282]\n",
      " [180.4119 ]\n",
      " [196.33688]\n",
      " [142.23404]]\n",
      "1460 Cost:  0.43406755 \n",
      "Prediction:\n",
      " [[150.65724]\n",
      " [185.17165]\n",
      " [180.41243]\n",
      " [196.33714]\n",
      " [142.23262]]\n",
      "1470 Cost:  0.43307766 \n",
      "Prediction:\n",
      " [[150.65892]\n",
      " [185.17049]\n",
      " [180.41295]\n",
      " [196.3374 ]\n",
      " [142.23122]]\n",
      "1480 Cost:  0.43208927 \n",
      "Prediction:\n",
      " [[150.66061]\n",
      " [185.16936]\n",
      " [180.4135 ]\n",
      " [196.33766]\n",
      " [142.22981]]\n",
      "1490 Cost:  0.43110147 \n",
      "Prediction:\n",
      " [[150.6623 ]\n",
      " [185.16818]\n",
      " [180.41402]\n",
      " [196.3379 ]\n",
      " [142.22841]]\n",
      "1500 Cost:  0.43011504 \n",
      "Prediction:\n",
      " [[150.664  ]\n",
      " [185.16705]\n",
      " [180.41457]\n",
      " [196.33818]\n",
      " [142.22704]]\n",
      "1510 Cost:  0.42914262 \n",
      "Prediction:\n",
      " [[150.66566]\n",
      " [185.16591]\n",
      " [180.41508]\n",
      " [196.33842]\n",
      " [142.22563]]\n",
      "1520 Cost:  0.42816576 \n",
      "Prediction:\n",
      " [[150.66736]\n",
      " [185.1648 ]\n",
      " [180.41562]\n",
      " [196.33868]\n",
      " [142.22426]]\n",
      "1530 Cost:  0.42720643 \n",
      "Prediction:\n",
      " [[150.66902]\n",
      " [185.16365]\n",
      " [180.41615]\n",
      " [196.33896]\n",
      " [142.22287]]\n",
      "1540 Cost:  0.4262413 \n",
      "Prediction:\n",
      " [[150.67068]\n",
      " [185.16249]\n",
      " [180.41667]\n",
      " [196.33919]\n",
      " [142.2215 ]]\n",
      "1550 Cost:  0.42528924 \n",
      "Prediction:\n",
      " [[150.67236]\n",
      " [185.16139]\n",
      " [180.41722]\n",
      " [196.33948]\n",
      " [142.22014]]\n",
      "1560 Cost:  0.42433318 \n",
      "Prediction:\n",
      " [[150.67401]\n",
      " [185.16026]\n",
      " [180.41772]\n",
      " [196.33969]\n",
      " [142.21875]]\n",
      "1570 Cost:  0.42338744 \n",
      "Prediction:\n",
      " [[150.67567]\n",
      " [185.15915]\n",
      " [180.41824]\n",
      " [196.33997]\n",
      " [142.2174 ]]\n",
      "1580 Cost:  0.42244992 \n",
      "Prediction:\n",
      " [[150.67732]\n",
      " [185.15804]\n",
      " [180.41878]\n",
      " [196.34023]\n",
      " [142.21603]]\n",
      "1590 Cost:  0.42150864 \n",
      "Prediction:\n",
      " [[150.67897]\n",
      " [185.1569 ]\n",
      " [180.4193 ]\n",
      " [196.34045]\n",
      " [142.21468]]\n",
      "1600 Cost:  0.42058387 \n",
      "Prediction:\n",
      " [[150.6806 ]\n",
      " [185.1558 ]\n",
      " [180.41982]\n",
      " [196.34071]\n",
      " [142.21332]]\n",
      "1610 Cost:  0.41965514 \n",
      "Prediction:\n",
      " [[150.68224]\n",
      " [185.1547 ]\n",
      " [180.42032]\n",
      " [196.34094]\n",
      " [142.21198]]\n",
      "1620 Cost:  0.41873103 \n",
      "Prediction:\n",
      " [[150.68388]\n",
      " [185.1536 ]\n",
      " [180.42085]\n",
      " [196.3412 ]\n",
      " [142.21063]]\n",
      "1630 Cost:  0.417816 \n",
      "Prediction:\n",
      " [[150.6855 ]\n",
      " [185.15248]\n",
      " [180.42136]\n",
      " [196.34145]\n",
      " [142.20927]]\n",
      "1640 Cost:  0.4168984 \n",
      "Prediction:\n",
      " [[150.68713]\n",
      " [185.15137]\n",
      " [180.42188]\n",
      " [196.34167]\n",
      " [142.20795]]\n",
      "1650 Cost:  0.4159936 \n",
      "Prediction:\n",
      " [[150.68875]\n",
      " [185.15027]\n",
      " [180.42238]\n",
      " [196.34193]\n",
      " [142.20662]]\n",
      "1660 Cost:  0.4150938 \n",
      "Prediction:\n",
      " [[150.69035]\n",
      " [185.14917]\n",
      " [180.42288]\n",
      " [196.34216]\n",
      " [142.20528]]\n",
      "1670 Cost:  0.41419616 \n",
      "Prediction:\n",
      " [[150.69199]\n",
      " [185.1481 ]\n",
      " [180.42342]\n",
      " [196.34244]\n",
      " [142.20396]]\n",
      "1680 Cost:  0.41330022 \n",
      "Prediction:\n",
      " [[150.69359]\n",
      " [185.147  ]\n",
      " [180.42392]\n",
      " [196.34265]\n",
      " [142.20264]]\n",
      "1690 Cost:  0.4124171 \n",
      "Prediction:\n",
      " [[150.69519]\n",
      " [185.14592]\n",
      " [180.42442]\n",
      " [196.34293]\n",
      " [142.20132]]\n",
      "1700 Cost:  0.41153002 \n",
      "Prediction:\n",
      " [[150.6968 ]\n",
      " [185.14484]\n",
      " [180.42493]\n",
      " [196.34315]\n",
      " [142.20001]]\n",
      "1710 Cost:  0.410649 \n",
      "Prediction:\n",
      " [[150.69838]\n",
      " [185.14374]\n",
      " [180.42543]\n",
      " [196.34337]\n",
      " [142.19868]]\n",
      "1720 Cost:  0.40977278 \n",
      "Prediction:\n",
      " [[150.69998]\n",
      " [185.14267]\n",
      " [180.42593]\n",
      " [196.34363]\n",
      " [142.19739]]\n",
      "1730 Cost:  0.40889844 \n",
      "Prediction:\n",
      " [[150.70157]\n",
      " [185.14159]\n",
      " [180.42644]\n",
      " [196.34384]\n",
      " [142.19608]]\n",
      "1740 Cost:  0.4080347 \n",
      "Prediction:\n",
      " [[150.70316]\n",
      " [185.14052]\n",
      " [180.42694]\n",
      " [196.3441 ]\n",
      " [142.19478]]\n",
      "1750 Cost:  0.40718025 \n",
      "Prediction:\n",
      " [[150.70473]\n",
      " [185.13945]\n",
      " [180.42744]\n",
      " [196.34435]\n",
      " [142.1935 ]]\n",
      "1760 Cost:  0.40631622 \n",
      "Prediction:\n",
      " [[150.7063 ]\n",
      " [185.13838]\n",
      " [180.42792]\n",
      " [196.34456]\n",
      " [142.19218]]\n",
      "1770 Cost:  0.40546474 \n",
      "Prediction:\n",
      " [[150.70789]\n",
      " [185.13734]\n",
      " [180.42844]\n",
      " [196.34482]\n",
      " [142.1909 ]]\n",
      "1780 Cost:  0.4046251 \n",
      "Prediction:\n",
      " [[150.70943]\n",
      " [185.13626]\n",
      " [180.42892]\n",
      " [196.34503]\n",
      " [142.18962]]\n",
      "1790 Cost:  0.4037861 \n",
      "Prediction:\n",
      " [[150.71098]\n",
      " [185.13521]\n",
      " [180.42943]\n",
      " [196.34526]\n",
      " [142.18834]]\n",
      "1800 Cost:  0.4029447 \n",
      "Prediction:\n",
      " [[150.71255]\n",
      " [185.13416]\n",
      " [180.42993]\n",
      " [196.3455 ]\n",
      " [142.18707]]\n",
      "1810 Cost:  0.40210614 \n",
      "Prediction:\n",
      " [[150.71411]\n",
      " [185.13312]\n",
      " [180.43042]\n",
      " [196.34572]\n",
      " [142.18579]]\n",
      "1820 Cost:  0.40126902 \n",
      "Prediction:\n",
      " [[150.71567]\n",
      " [185.13203]\n",
      " [180.4309 ]\n",
      " [196.34596]\n",
      " [142.18451]]\n",
      "1830 Cost:  0.4004426 \n",
      "Prediction:\n",
      " [[150.71722]\n",
      " [185.13101]\n",
      " [180.4314 ]\n",
      " [196.34619]\n",
      " [142.18326]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1840 Cost:  0.39962834 \n",
      "Prediction:\n",
      " [[150.71875]\n",
      " [185.12997]\n",
      " [180.43188]\n",
      " [196.34642]\n",
      " [142.18198]]\n",
      "1850 Cost:  0.39880428 \n",
      "Prediction:\n",
      " [[150.72032]\n",
      " [185.12895]\n",
      " [180.4324 ]\n",
      " [196.34666]\n",
      " [142.18074]]\n",
      "1860 Cost:  0.39799327 \n",
      "Prediction:\n",
      " [[150.72182]\n",
      " [185.12787]\n",
      " [180.43285]\n",
      " [196.34685]\n",
      " [142.17946]]\n",
      "1870 Cost:  0.39718857 \n",
      "Prediction:\n",
      " [[150.72336]\n",
      " [185.12685]\n",
      " [180.43335]\n",
      " [196.3471 ]\n",
      " [142.17822]]\n",
      "1880 Cost:  0.39638138 \n",
      "Prediction:\n",
      " [[150.72487]\n",
      " [185.1258 ]\n",
      " [180.4338 ]\n",
      " [196.3473 ]\n",
      " [142.17696]]\n",
      "1890 Cost:  0.3955833 \n",
      "Prediction:\n",
      " [[150.7264 ]\n",
      " [185.12477]\n",
      " [180.43431]\n",
      " [196.34753]\n",
      " [142.1757 ]]\n",
      "1900 Cost:  0.39477983 \n",
      "Prediction:\n",
      " [[150.72794]\n",
      " [185.12376]\n",
      " [180.4348 ]\n",
      " [196.34776]\n",
      " [142.17448]]\n",
      "1910 Cost:  0.3939887 \n",
      "Prediction:\n",
      " [[150.72945]\n",
      " [185.12273]\n",
      " [180.43527]\n",
      " [196.34799]\n",
      " [142.17325]]\n",
      "1920 Cost:  0.39320546 \n",
      "Prediction:\n",
      " [[150.73094]\n",
      " [185.1217 ]\n",
      " [180.43576]\n",
      " [196.34819]\n",
      " [142.172  ]]\n",
      "1930 Cost:  0.39241537 \n",
      "Prediction:\n",
      " [[150.73247]\n",
      " [185.1207 ]\n",
      " [180.43623]\n",
      " [196.34843]\n",
      " [142.17078]]\n",
      "1940 Cost:  0.39163893 \n",
      "Prediction:\n",
      " [[150.73395]\n",
      " [185.11966]\n",
      " [180.43669]\n",
      " [196.34863]\n",
      " [142.16954]]\n",
      "1950 Cost:  0.3908689 \n",
      "Prediction:\n",
      " [[150.73544]\n",
      " [185.11865]\n",
      " [180.43718]\n",
      " [196.34886]\n",
      " [142.16832]]\n",
      "1960 Cost:  0.39009532 \n",
      "Prediction:\n",
      " [[150.73694]\n",
      " [185.11763]\n",
      " [180.43765]\n",
      " [196.34908]\n",
      " [142.1671 ]]\n",
      "1970 Cost:  0.3893168 \n",
      "Prediction:\n",
      " [[150.73845]\n",
      " [185.11662]\n",
      " [180.43813]\n",
      " [196.34929]\n",
      " [142.16588]]\n",
      "1980 Cost:  0.38856453 \n",
      "Prediction:\n",
      " [[150.73991]\n",
      " [185.11563]\n",
      " [180.43858]\n",
      " [196.34952]\n",
      " [142.16467]]\n",
      "1990 Cost:  0.38779765 \n",
      "Prediction:\n",
      " [[150.74141]\n",
      " [185.11462]\n",
      " [180.43906]\n",
      " [196.34973]\n",
      " [142.16345]]\n",
      "2000 Cost:  0.38704437 \n",
      "Prediction:\n",
      " [[150.74287]\n",
      " [185.11362]\n",
      " [180.43951]\n",
      " [196.34993]\n",
      " [142.16225]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x_data = [[73., 80., 75.],[93., 88., 93.],[89., 91., 90.],[96., 98., 100.],[73., 66., 70.]]\n",
    "y_data = [[152.],[185.],[180.],[196.],[142.]]\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "# hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b # tf.matmul은 행렬곱\n",
    "# simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict = {X: x_data, Y: y_data})\n",
    "    if step % 10 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic(regression) Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.7928755\n",
      "200 0.6353347\n",
      "400 0.60983324\n",
      "600 0.5876545\n",
      "800 0.56629825\n",
      "1000 0.54576993\n",
      "1200 0.52607006\n",
      "1400 0.5071941\n",
      "1600 0.48913276\n",
      "1800 0.47187248\n",
      "2000 0.45539585\n",
      "2200 0.43968198\n",
      "2400 0.4247073\n",
      "2600 0.4104463\n",
      "2800 0.3968716\n",
      "3000 0.38395497\n",
      "3200 0.37166724\n",
      "3400 0.3599795\n",
      "3600 0.34886253\n",
      "3800 0.33828768\n",
      "4000 0.3282268\n",
      "4200 0.31865284\n",
      "4400 0.3095392\n",
      "4600 0.3008609\n",
      "4800 0.2925935\n",
      "5000 0.28471416\n",
      "5200 0.27720076\n",
      "5400 0.27003282\n",
      "5600 0.26319054\n",
      "5800 0.25665554\n",
      "6000 0.2504104\n",
      "6200 0.24443848\n",
      "6400 0.23872454\n",
      "6600 0.23325408\n",
      "6800 0.22801358\n",
      "7000 0.22299021\n",
      "7200 0.21817203\n",
      "7400 0.21354793\n",
      "7600 0.20910738\n",
      "7800 0.20484054\n",
      "8000 0.20073813\n",
      "8200 0.19679146\n",
      "8400 0.19299255\n",
      "8600 0.18933369\n",
      "8800 0.18580782\n",
      "9000 0.18240815\n",
      "9200 0.17912847\n",
      "9400 0.17596276\n",
      "9600 0.17290561\n",
      "9800 0.16995168\n",
      "10000 0.16709615\n",
      "\n",
      "Hypothesis:  [[0.0384752 ]\n",
      " [0.16816539]\n",
      " [0.339973  ]\n",
      " [0.76577187]\n",
      " [0.92931575]\n",
      " [0.9767899 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]] # 0 은 fail, 1 은 pass\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape = [None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "# hypothesisy using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X,W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W)+b)\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1- Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis >0.5 , dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val,_ = sess.run([cost, train], feed_dict= {X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "    \n",
    "    h, c, a= sess.run([hypothesis, predicted, accuracy], feed_dict= {X:x_data,Y:y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.372783\n",
      "200 0.53314674\n",
      "400 0.4342449\n",
      "600 0.35606107\n",
      "800 0.28073943\n",
      "1000 0.23565112\n",
      "1200 0.21317962\n",
      "1400 0.19455725\n",
      "1600 0.17885546\n",
      "1800 0.16543518\n",
      "2000 0.15383619\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5],[1, 7, 5, 5],[1, 2, 5, 6],[1, 6, 6, 6],[1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],[0, 0, 1],[0, 0, 1],[0, 1, 0],[0, 1, 0,],[0, 1, 0],[1, 0, 0],[1, 0, 0 ]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name = \"bias\")\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(Logits) / reduce_sum(exp(Logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/Loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-dda5b4dec856>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-dda5b4dec856>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    logits= tf.matmul(X, W) + b\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# # 위에것을 fancy하게 구현하기(다른데이터사용)\n",
    "# #파일불러오는 거 가정해서하는것\n",
    "# xy = np.loadtxt('data-04-zoo.csv', delimiter = ',', dtype= np.float32)\n",
    "# x_data = xy[:,0:-1]\n",
    "# y_data = xy[:,[-1]]\n",
    "\n",
    "# nb_classes = 7\n",
    "\n",
    "# X = tf.placeholder(tf.float32, [None, 16])\n",
    "# Y = tf.placeholder(tf.int32, [None, 1]) #0 ~ 6\n",
    "\n",
    "# Y_one_hot = tf.one_hot(Y, nb_classes)\n",
    "# Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "\n",
    "# W = tf.Variable(tf.random_normal([16, nb_classes]), name = 'weight')\n",
    "# b = tf.Variable(tf.random_normal([nb_classes]), name = 'bias'\n",
    "\n",
    "# # tf.nn.softmax computes softmax activations\n",
    "# #softmax = exp(Logits) / reduce_sum(exp(Logits),dim)\n",
    "# logits= tf.matmul(X, W) + b\n",
    "# hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# # Cross entropy cost/Loss\n",
    "# cost_i = tf.nn.softmax_cross_entropy_with_logits(logits= logits, labels = Y_one_hot)\n",
    "\n",
    "# cost = tf.reduce_mean(cost_i)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate =0.1).minimize(cost)\n",
    "\n",
    "# prediction = tf.argmax(hypothesis, 1)\n",
    "# correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                \n",
    "# # Launch graph\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     for step in range(2000):\n",
    "#         sess.run(optimizer, feed_dict = {X: x_data, Y: y_data})\n",
    "#         if step % 100 ==0:\n",
    "#             loss, acc = sess.run([cost, accuracy], feed_dict={X: x_data, Y:y_data})\n",
    "#             print(\"step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n",
    "    \n",
    "#     # Let's see if we can predict            \n",
    "#     pred = sess.run(prediction, feed_dict={X: x_data})\n",
    "#     # y_dat: (N,1) = flatten => (N,) matches pred.shape\n",
    "#     for p, y in zip(pred, y_data.flatten()):\n",
    "#         prit(\"[{}] prediction: {} True Y:{}\".format(p == int(y), p, int(y)))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-dfb0edab3ff8>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "read_data_sets() missing 1 required positional argument: 'train_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dfb0edab3ff8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Check out https://www.tensorflow.org/get_started/mnist/beginners for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# more information about the mnist dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: read_data_sets() missing 1 required positional argument: 'train_dir'"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "\n",
    "print(\"Accuracy: \", accuracy.eval(session = sess, feed_dict = {X:mnist.test.images, Y:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0., 1., 2., 3., 4., 5., 6.])\n",
      "1\n",
      "(7,)\n",
      "0.0 1.0 6.0\n",
      "[2. 3. 4.]\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# simple 1D array and slicing\n",
    "import numpy as np\n",
    "import pprint as pp # 출력물 좀 더 이쁘게 배열 해준다.\n",
    "\n",
    "t = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "pp.pprint(t)\n",
    "print(t.ndim) # rank(차원의 수)\n",
    "print(t.shape) # shape\n",
    "print(t[0], t[1], t[-1])\n",
    "print(t[2:5])\n",
    "print(t[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.,  2.,  3.],\n",
      "       [ 4.,  5.,  6.],\n",
      "       [ 7.,  8.,  9.],\n",
      "       [10., 11., 12.]])\n",
      "2\n",
      "(4, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  2.,  3.]],\n",
       "\n",
       "       [[ 4.,  5.,  6.]],\n",
       "\n",
       "       [[ 7.,  8.,  9.]],\n",
       "\n",
       "       [[10., 11., 12.]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2D array\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "\n",
    "t = np.array([[1., 2., 3.],[4., 5., 6.], [7., 8., 9.],[10., 11., 12.]])\n",
    "pp.pprint(t)\n",
    "print(t.ndim)\n",
    "print(t.shape)\n",
    "t.reshape(4,1,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬곱 일반곱\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "#행렬곱\n",
    "\n",
    "matrix1 = tf.constant([[1., 2.],[3., 4.]])\n",
    "matrix2 = tf.constant([[1.],[2.]])\n",
    "print(\"Metrix 1 shape\", matrix1.shape)\n",
    "print(\"Metrix 2 shape\", matrix2.shape)\n",
    "tf.matmul(matrix1, matrix2).eval(session=sess)\n",
    "#일반곱\n",
    "(matrix1 * matrix2).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Broadcasting(shape이 달라도 계산가능 1차원)\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "\n",
    "matrix1 = tf.constant([[1.,2.]])\n",
    "matrix2 = tf.constant(3.) # 이렇게 브로드캐스팅이용할 수 있지만 이왕이면 같은차원으로 하는게 가독성 좋다\n",
    "(matrix1 + matrix2).eval(session=sess)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Reduce mean(값이 여러개인 행렬을 줄여서 평균을 구하는 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 3.5], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "# tf.reduce_mean([1,2], axis = 0).eval(session= sess) # float 이아니라 int여서 1로 출력\n",
    "# >>>1\n",
    "x= [[1., 2.],\n",
    "    [3., 4.]]\n",
    "#tf.reduce_mean(x, axis =0).eval(session=sess)\n",
    "#tf.reduce_mean(x, axis =1).eval(session=sess)\n",
    "#tf.reduce_mean(x, axis =-1).eval(session=sess) # axis = -1 가장안쪽에 있는것 평균"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "x = [[1.,2.],\n",
    "     [3., 4.]]\n",
    "\n",
    "# tf.reduce_sum(x).eval(session = sess)\n",
    "# tf.reduce_sum(x, axis = 0).eval(session = sess)\n",
    "# tf.reduce_sum(x, axis = -1).eval(session = sess)\n",
    "# tf.reduce_mean(tf.reduce_sum(x, axis = -1)).eval(session = sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[0, 1, 2],\n",
    "     [2, 1, 0]]\n",
    "# tf.argmax(x, axis = 0).eval(session = sess) # 축기준 멕시멈 값의 위치 출력\n",
    "# tf.argmax(x, axis = 1).eval(session = sess)\n",
    "# tf.argmax(x, axis = -1).eval(session = sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "             \n",
    "             [[6, 7, 8],\n",
    "              [9, 10, 11]]])\n",
    "#pp.pprint(t)\n",
    "# t.shape\n",
    "\n",
    "# tf.reshape(t, shape = [-1, 3]).eval(session = sess)\n",
    "# tf.reshape(t, shape = [-1, 1, 3]).eval(session = sess)\n",
    "\n",
    "# reshape()의 '-1'이 의미하는 바는, 변경된 배열의 '-1' 위치의 차원은 \"원래 배열의 길이와 남은 차원으로 부터 추정\"이 된다는 뜻입니다.\n",
    "\n",
    "# Reshape( squeeze, expand)\n",
    "tf.squeeze([[0],[1],[2]]).eval(session = sess)\n",
    "tf.expand_dims([0, 1, 2],1).eval(session = sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "tf.one_hot([[0],[1],[2],[0]], depth = 3).eval(session = sess)\n",
    "# 자동으로 rank하나를 expand함\n",
    "#그게 싫으면 reshape하면된다.\n",
    "# t = tf.one_hot([[0],[1],[2],[0]], depth = 3)\n",
    "# tf.reshape(t,shape = [-1, 3]).eval(session = sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "tf.cast([1.8, 2.2, 3.3, 4.9], tf.int32).eval(session = sess) #주어진 tensor를 int로 바꿈\n",
    "tf.cast([True, False, 1 == 1, 0 ==1], tf.int32).eval(session = sess) # 불리안을 0이나 1로 바꿈\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "#stack 은 쌓는다는 의미\n",
    "\n",
    "x = [1, 4]\n",
    "y = [2, 5]\n",
    "z = [3, 6]\n",
    "\n",
    "#pack along first dim.\n",
    "tf.stack([x, y, z]).eval(session = sess)\n",
    "tf.stack([x, y, z], axis = 1).eval(session = sess) #축 기준으로 바꿔서 쌓음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ones and Zeros like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "x = [[0, 1, 2],\n",
    "     [2, 1, 0]]\n",
    "\n",
    "tf.ones_like(x).eval(session = sess)\n",
    "tf.zeros_like(x).eval(session = sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n",
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "for x, y in zip([1, 2, 3],[4, 5, 6]):\n",
    "    print(x, y)\n",
    "    \n",
    "for x, y, z in zip([1, 2, 3],[4, 5, 6],[7, 8, 9]):\n",
    "    print(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.74922144 [[ 1.3445129 ]\n",
      " [-0.11669051]]\n",
      "100 0.71248734 [[ 0.7768897 ]\n",
      " [-0.01649155]]\n",
      "200 0.7003921 [[0.47027746]\n",
      " [0.04421406]]\n",
      "300 0.6959808 [[0.28801355]\n",
      " [0.0600372 ]]\n",
      "400 0.69430256 [[0.1785013 ]\n",
      " [0.05665189]]\n",
      "500 0.69363457 [[0.11194577]\n",
      " [0.04684164]]\n",
      "600 0.69335806 [[0.07098859]\n",
      " [0.03620718]]\n",
      "700 0.69324005 [[0.04547017]\n",
      " [0.02688913]]\n",
      "800 0.6931886 [[0.029384  ]\n",
      " [0.01945772]]\n",
      "900 0.6931657 [[0.01913487]\n",
      " [0.0138321 ]]\n",
      "1000 0.6931555 [[0.01254225]\n",
      " [0.00970943]]\n",
      "1100 0.693151 [[0.00826623]\n",
      " [0.00675291]]\n",
      "1200 0.69314885 [[0.00547294]\n",
      " [0.00466448]]\n",
      "1300 0.69314796 [[0.00363716]\n",
      " [0.00320524]]\n",
      "1400 0.6931476 [[0.00242456]\n",
      " [0.00219382]]\n",
      "1500 0.6931473 [[0.00162025]\n",
      " [0.00149698]]\n",
      "1600 0.6931473 [[0.00108491]\n",
      " [0.00101906]]\n",
      "1700 0.6931472 [[0.00072764]\n",
      " [0.00069247]]\n",
      "1800 0.6931472 [[0.0004887 ]\n",
      " [0.00046991]]\n",
      "1900 0.6931472 [[0.00032853]\n",
      " [0.00031849]]\n",
      "2000 0.69314724 [[0.00022104]\n",
      " [0.00021567]]\n",
      "2100 0.6931472 [[0.0001488 ]\n",
      " [0.00014593]]\n",
      "2200 0.6931472 [[1.0023104e-04]\n",
      " [9.8693898e-05]]\n",
      "2300 0.6931472 [[6.7539375e-05]\n",
      " [6.6718967e-05]]\n",
      "2400 0.6931472 [[4.5522913e-05]\n",
      " [4.5085460e-05]]\n",
      "2500 0.6931472 [[3.0690295e-05]\n",
      " [3.0455501e-05]]\n",
      "2600 0.6931472 [[2.0691612e-05]\n",
      " [2.0567093e-05]]\n",
      "2700 0.6931472 [[1.3960756e-05]\n",
      " [1.3892858e-05]]\n",
      "2800 0.69314724 [[9.417395e-06]\n",
      " [9.377810e-06]]\n",
      "2900 0.6931472 [[6.3492485e-06]\n",
      " [6.3305238e-06]]\n",
      "3000 0.6931472 [[4.289904e-06]\n",
      " [4.281611e-06]]\n",
      "3100 0.6931472 [[2.8623717e-06]\n",
      " [2.8615295e-06]]\n",
      "3200 0.6931472 [[1.9608492e-06]\n",
      " [1.9600070e-06]]\n",
      "3300 0.6931472 [[1.3022166e-06]\n",
      " [1.3013744e-06]]\n",
      "3400 0.6931472 [[8.834907e-07]\n",
      " [8.826485e-07]]\n",
      "3500 0.69314724 [[5.7056565e-07]\n",
      " [5.6972345e-07]]\n",
      "3600 0.6931472 [[3.9473196e-07]\n",
      " [3.9388976e-07]]\n",
      "3700 0.6931472 [[2.4870002e-07]\n",
      " [2.4785783e-07]]\n",
      "3800 0.6931472 [[1.7121369e-07]\n",
      " [1.7037149e-07]]\n",
      "3900 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "4000 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "4100 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "4200 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "4300 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "4400 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "4500 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "4600 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "4700 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "4800 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "4900 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "5000 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "5100 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "5200 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "5300 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "5400 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "5500 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "5600 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "5700 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "5800 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "5900 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "6000 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "6100 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "6200 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "6300 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "6400 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "6500 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "6600 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "6700 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "6800 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "6900 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "7000 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "7100 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "7200 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "7300 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "7400 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "7500 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "7600 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "7700 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "7800 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "7900 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "8000 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "8100 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "8200 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "8300 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "8400 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "8500 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "8600 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "8700 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "8800 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "8900 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "9000 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "9100 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "9200 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "9300 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "9400 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "9500 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "9600 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "9700 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "9800 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "9900 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "10000 0.6931472 [[1.3396064e-07]\n",
      " [1.3311845e-07]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0],    [1],    [1],    [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X,W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) +b)\n",
    "\n",
    "# Cost/loss function\n",
    "cost = -tf.reduce_mean(Y *tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize Tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 셋 이 4개밖에 안되는데 accuracy가 0.5 밖에 안나옴...\n",
    "이걸 되게 하려면 이때 뉴렬 넷 을 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7007551 [array([[-0.6693347, -0.6412805],\n",
      "       [-1.435883 , -0.8319943]], dtype=float32), array([[ 0.8716725],\n",
      "       [-0.3947619]], dtype=float32)]\n",
      "100 0.6967011 [array([[-0.5422977 , -0.68333906],\n",
      "       [-1.3703836 , -0.8723788 ]], dtype=float32), array([[ 0.7670027 ],\n",
      "       [-0.45721236]], dtype=float32)]\n",
      "200 0.6937382 [array([[-0.43765873, -0.7331414 ],\n",
      "       [-1.3207431 , -0.919663  ]], dtype=float32), array([[ 0.69388866],\n",
      "       [-0.52156913]], dtype=float32)]\n",
      "300 0.6912619 [array([[-0.3511146 , -0.7918616 ],\n",
      "       [-1.2834553 , -0.97490436]], dtype=float32), array([[ 0.6432176],\n",
      "       [-0.5923993]], dtype=float32)]\n",
      "400 0.68885297 [array([[-0.27887973, -0.8611088 ],\n",
      "       [-1.2557226 , -1.0396543 ]], dtype=float32), array([[ 0.6096426 ],\n",
      "       [-0.67219454]], dtype=float32)]\n",
      "500 0.68618286 [array([[-0.21803847, -0.9424965 ],\n",
      "       [-1.2355623 , -1.115565  ]], dtype=float32), array([[ 0.589435 ],\n",
      "       [-0.7634761]], dtype=float32)]\n",
      "600 0.6829666 [array([[-0.16644153, -1.0374123 ],\n",
      "       [-1.2216359 , -1.2041804 ]], dtype=float32), array([[ 0.5800475],\n",
      "       [-0.8687659]], dtype=float32)]\n",
      "700 0.6789441 [array([[-0.12255  , -1.146758 ],\n",
      "       [-1.2130866, -1.3066865]], dtype=float32), array([[ 0.579835 ],\n",
      "       [-0.9903605]], dtype=float32)]\n",
      "800 0.673881 [array([[-0.0852954, -1.270736 ],\n",
      "       [-1.2094247, -1.4236844]], dtype=float32), array([[ 0.58786374],\n",
      "       [-1.130027  ]], dtype=float32)]\n",
      "900 0.66757613 [array([[-0.05397743, -1.4088033 ],\n",
      "       [-1.2104484 , -1.5551057 ]], dtype=float32), array([[ 0.6037837],\n",
      "       [-1.2887226]], dtype=float32)]\n",
      "1000 0.6598675 [array([[-0.02820298, -1.559823  ],\n",
      "       [-1.2162125 , -1.7003008 ]], dtype=float32), array([[ 0.62772924],\n",
      "       [-1.4664259 ]], dtype=float32)]\n",
      "1100 0.65063936 [array([[-0.00785793, -1.7223026 ],\n",
      "       [-1.227009  , -1.8582298 ]], dtype=float32), array([[ 0.6602188],\n",
      "       [-1.6621039]], dtype=float32)]\n",
      "1200 0.63984746 [array([[ 0.00691071, -1.8945594 ],\n",
      "       [-1.2433578 , -2.0276053 ]], dtype=float32), array([[ 0.7020241],\n",
      "       [-1.8737346]], dtype=float32)]\n",
      "1300 0.6275628 [array([[ 0.01573505, -2.0747046 ],\n",
      "       [-1.2659593 , -2.206876  ]], dtype=float32), array([[ 0.75400454],\n",
      "       [-2.0983462 ]], dtype=float32)]\n",
      "1400 0.6140096 [array([[ 0.01811943, -2.2605133 ],\n",
      "       [-1.2955765 , -2.3940938 ]], dtype=float32), array([[ 0.8169167],\n",
      "       [-2.3321395]], dtype=float32)]\n",
      "1500 0.5995576 [array([[ 0.01359184, -2.4493341 ],\n",
      "       [-1.3328438 , -2.5868142 ]], dtype=float32), array([[ 0.8912561],\n",
      "       [-2.5707965]], dtype=float32)]\n",
      "1600 0.58465266 [array([[ 1.8678074e-03, -2.6382012e+00],\n",
      "       [-1.3780260e+00, -2.7821989e+00]], dtype=float32), array([[ 0.9771761],\n",
      "       [-2.8099952]], dtype=float32)]\n",
      "1700 0.56971693 [array([[-0.01706656, -2.8241527 ],\n",
      "       [-1.4308085 , -2.9773185 ]], dtype=float32), array([[ 1.0744891],\n",
      "       [-3.0459754]], dtype=float32)]\n",
      "1800 0.5550685 [array([[-0.04303496, -3.0046034 ],\n",
      "       [-1.4901948 , -3.169541  ]], dtype=float32), array([[ 1.1827128],\n",
      "       [-3.2759426]], dtype=float32)]\n",
      "1900 0.54088795 [array([[-0.07591944, -3.1776085 ],\n",
      "       [-1.5545278 , -3.3568146 ]], dtype=float32), array([[ 1.301129 ],\n",
      "       [-3.4982061]], dtype=float32)]\n",
      "2000 0.52722526 [array([[-0.11599666, -3.3419495 ],\n",
      "       [-1.6215891 , -3.5377667 ]], dtype=float32), array([[ 1.4288707],\n",
      "       [-3.7120874]], dtype=float32)]\n",
      "2100 0.5140193 [array([[-0.16425617, -3.4970784 ],\n",
      "       [-1.6886889 , -3.711635  ]], dtype=float32), array([[ 1.5650487],\n",
      "       [-3.917705 ]], dtype=float32)]\n",
      "2200 0.50110954 [array([[-0.22263643, -3.6429884 ],\n",
      "       [-1.752708  , -3.8781147 ]], dtype=float32), array([[ 1.7089398],\n",
      "       [-4.1157494]], dtype=float32)]\n",
      "2300 0.48822367 [array([[-0.2941764, -3.7800825],\n",
      "       [-1.810113 , -4.0371695]], dtype=float32), array([[ 1.8602355],\n",
      "       [-4.30728  ]], dtype=float32)]\n",
      "2400 0.47493172 [array([[-0.3830955, -3.9090672],\n",
      "       [-1.8570735, -4.1888475]], dtype=float32), array([[ 2.0193703],\n",
      "       [-4.493577 ]], dtype=float32)]\n",
      "2500 0.46056858 [array([[-0.49472797, -4.0308843 ],\n",
      "       [-1.8899717 , -4.333098  ]], dtype=float32), array([[ 2.18792  ],\n",
      "       [-4.6760063]], dtype=float32)]\n",
      "2600 0.4441794 [array([[-0.63494396, -4.1466727 ],\n",
      "       [-1.9068986 , -4.4695935 ]], dtype=float32), array([[ 2.3689406],\n",
      "       [-4.855845 ]], dtype=float32)]\n",
      "2700 0.42466158 [array([[-0.80822945, -4.2577305 ],\n",
      "       [-1.9106671 , -4.597631  ]], dtype=float32), array([[ 2.5668614],\n",
      "       [-5.034025 ]], dtype=float32)]\n",
      "2800 0.40131602 [array([[-1.0138265, -4.365384 ],\n",
      "       [-1.9119388, -4.7162685]], dtype=float32), array([[ 2.7861981],\n",
      "       [-5.2108994]], dtype=float32)]\n",
      "2900 0.37449977 [array([[-1.2422369, -4.4706445],\n",
      "       [-1.9272754, -4.82482  ]], dtype=float32), array([[ 3.0290294],\n",
      "       [-5.386329 ]], dtype=float32)]\n",
      "3000 0.34550738 [array([[-1.4774543, -4.5737514],\n",
      "       [-1.9702253, -4.9233527]], dtype=float32), array([[ 3.292891],\n",
      "       [-5.560132]], dtype=float32)]\n",
      "3100 0.3158114 [array([[-1.7046144, -4.6740675],\n",
      "       [-2.044439 , -5.0126743]], dtype=float32), array([[ 3.5711944],\n",
      "       [-5.7322407]], dtype=float32)]\n",
      "3200 0.28661096 [array([[-1.9153055, -4.7704806],\n",
      "       [-2.1448922, -5.093933 ]], dtype=float32), array([[ 3.8557763],\n",
      "       [-5.9023757]], dtype=float32)]\n",
      "3300 0.25880378 [array([[-2.107193 , -4.8619404],\n",
      "       [-2.2627478, -5.1682696]], dtype=float32), array([[ 4.139233 ],\n",
      "       [-6.0698442]], dtype=float32)]\n",
      "3400 0.23302396 [array([[-2.2811844, -4.947767 ],\n",
      "       [-2.3891985, -5.23665  ]], dtype=float32), array([[ 4.4159107],\n",
      "       [-6.2337008]], dtype=float32)]\n",
      "3500 0.20964608 [array([[-2.439179 , -5.0277047],\n",
      "       [-2.5173213, -5.299832 ]], dtype=float32), array([[ 4.681959 ],\n",
      "       [-6.3930187]], dtype=float32)]\n",
      "3600 0.18880308 [array([[-2.583052 , -5.101834 ],\n",
      "       [-2.642471 , -5.3584094]], dtype=float32), array([[ 4.9350524],\n",
      "       [-6.5470557]], dtype=float32)]\n",
      "3700 0.17043886 [array([[-2.714408 , -5.170453 ],\n",
      "       [-2.7619367, -5.412864 ]], dtype=float32), array([[ 5.1740527],\n",
      "       [-6.69533  ]], dtype=float32)]\n",
      "3800 0.15437587 [array([[-2.8346055, -5.23397  ],\n",
      "       [-2.874397 , -5.463607 ]], dtype=float32), array([[ 5.3986883],\n",
      "       [-6.8375955]], dtype=float32)]\n",
      "3900 0.14037609 [array([[-2.9448264, -5.292838 ],\n",
      "       [-2.979422 , -5.5109963]], dtype=float32), array([[ 5.6092825],\n",
      "       [-6.973798 ]], dtype=float32)]\n",
      "4000 0.1281842 [array([[-3.0461245, -5.347505 ],\n",
      "       [-3.077104 , -5.55535  ]], dtype=float32), array([[ 5.806499 ],\n",
      "       [-7.1040277]], dtype=float32)]\n",
      "4100 0.11755423 [array([[-3.139447 , -5.398388 ],\n",
      "       [-3.1678119, -5.596955 ]], dtype=float32), array([[ 5.9912014],\n",
      "       [-7.22847  ]], dtype=float32)]\n",
      "4200 0.108262405 [array([[-3.2256398, -5.445873 ],\n",
      "       [-3.252047 , -5.6360664]], dtype=float32), array([[ 6.164331],\n",
      "       [-7.347372]], dtype=float32)]\n",
      "4300 0.10011238 [array([[-3.3054624, -5.490303 ],\n",
      "       [-3.3303516, -5.6729198]], dtype=float32), array([[ 6.326833],\n",
      "       [-7.461015]], dtype=float32)]\n",
      "4400 0.09293556 [array([[-3.3795905, -5.5319867],\n",
      "       [-3.4032648, -5.7077174]], dtype=float32), array([[ 6.4796233],\n",
      "       [-7.569691 ]], dtype=float32)]\n",
      "4500 0.08658913 [array([[-3.448618 , -5.571192 ],\n",
      "       [-3.4712944, -5.7406445]], dtype=float32), array([[ 6.623541 ],\n",
      "       [-7.6737037]], dtype=float32)]\n",
      "4600 0.080952555 [array([[-3.5130725, -5.6081586],\n",
      "       [-3.5349078, -5.771864 ]], dtype=float32), array([[ 6.7593746],\n",
      "       [-7.7733383]], dtype=float32)]\n",
      "4700 0.075924866 [array([[-3.5734153, -5.643096 ],\n",
      "       [-3.5945277, -5.8015227]], dtype=float32), array([[ 6.887835],\n",
      "       [-7.868874]], dtype=float32)]\n",
      "4800 0.07142114 [array([[-3.6300523, -5.676188 ],\n",
      "       [-3.6505344, -5.8297505]], dtype=float32), array([[ 7.0095644],\n",
      "       [-7.9605727]], dtype=float32)]\n",
      "4900 0.06737034 [array([[-3.6833417, -5.707598 ],\n",
      "       [-3.7032642, -5.8566647]], dtype=float32), array([[ 7.1251388],\n",
      "       [-8.048681 ]], dtype=float32)]\n",
      "5000 0.06371255 [array([[-3.7335975, -5.7374725],\n",
      "       [-3.753017 , -5.8823667]], dtype=float32), array([[ 7.235074],\n",
      "       [-8.133432]], dtype=float32)]\n",
      "5100 0.0603972 [array([[-3.7810974, -5.765937 ],\n",
      "       [-3.8000607, -5.906953 ]], dtype=float32), array([[ 7.339832],\n",
      "       [-8.215036]], dtype=float32)]\n",
      "5200 0.057381447 [array([[-3.8260841, -5.7931066],\n",
      "       [-3.8446364, -5.9305058]], dtype=float32), array([[ 7.439832],\n",
      "       [-8.293693]], dtype=float32)]\n",
      "5300 0.05462894 [array([[-3.8687792, -5.8190823],\n",
      "       [-3.8869467, -5.953101 ]], dtype=float32), array([[ 7.535448],\n",
      "       [-8.369581]], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400 0.052108563 [array([[-3.9093733, -5.8439546],\n",
      "       [-3.9271839, -5.9748054]], dtype=float32), array([[ 7.6270113],\n",
      "       [-8.442874 ]], dtype=float32)]\n",
      "5500 0.049793772 [array([[-3.9480379, -5.867807 ],\n",
      "       [-3.9655156, -5.9956827]], dtype=float32), array([[ 7.7148232],\n",
      "       [-8.513723 ]], dtype=float32)]\n",
      "5600 0.04766173 [array([[-3.984925 , -5.8907104],\n",
      "       [-4.002093 , -6.0157857]], dtype=float32), array([[ 7.7991567],\n",
      "       [-8.582275 ]], dtype=float32)]\n",
      "5700 0.045692608 [array([[-4.0201735, -5.9127316],\n",
      "       [-4.037047 , -6.0351667]], dtype=float32), array([[ 7.880255],\n",
      "       [-8.648663]], dtype=float32)]\n",
      "5800 0.043869257 [array([[-4.0539055, -5.9339304],\n",
      "       [-4.070504 , -6.0538716]], dtype=float32), array([[ 7.9583416],\n",
      "       [-8.713006 ]], dtype=float32)]\n",
      "5900 0.042176872 [array([[-4.086231 , -5.9543614],\n",
      "       [-4.1025696, -6.0719423]], dtype=float32), array([[ 8.033617],\n",
      "       [-8.775417]], dtype=float32)]\n",
      "6000 0.04060233 [array([[-4.1172504, -5.9740744],\n",
      "       [-4.1333427, -6.089417 ]], dtype=float32), array([[ 8.106266],\n",
      "       [-8.836005]], dtype=float32)]\n",
      "6100 0.03913425 [array([[-4.147055 , -5.9931135],\n",
      "       [-4.1629124, -6.10633  ]], dtype=float32), array([[ 8.176456],\n",
      "       [-8.894866]], dtype=float32)]\n",
      "6200 0.03776275 [array([[-4.1757245, -6.011519 ],\n",
      "       [-4.1913586, -6.1227155]], dtype=float32), array([[ 8.244334],\n",
      "       [-8.952087]], dtype=float32)]\n",
      "6300 0.036478832 [array([[-4.203334 , -6.02933  ],\n",
      "       [-4.2187552, -6.138602 ]], dtype=float32), array([[ 8.310041],\n",
      "       [-9.007755]], dtype=float32)]\n",
      "6400 0.03527467 [array([[-4.2299495, -6.0465794],\n",
      "       [-4.2451677, -6.154018 ]], dtype=float32), array([[ 8.373706],\n",
      "       [-9.061946]], dtype=float32)]\n",
      "6500 0.03414339 [array([[-4.2556357, -6.063301 ],\n",
      "       [-4.2706594, -6.168988 ]], dtype=float32), array([[ 8.435446],\n",
      "       [-9.114734]], dtype=float32)]\n",
      "6600 0.03307875 [array([[-4.2804494, -6.079522 ],\n",
      "       [-4.2952847, -6.1835346]], dtype=float32), array([[ 8.495365],\n",
      "       [-9.166183]], dtype=float32)]\n",
      "6700 0.03207515 [array([[-4.304441, -6.095271],\n",
      "       [-4.319095, -6.197681]], dtype=float32), array([[ 8.553569],\n",
      "       [-9.21636 ]], dtype=float32)]\n",
      "6800 0.031127801 [array([[-4.327657 , -6.110571 ],\n",
      "       [-4.342138 , -6.2114453]], dtype=float32), array([[ 8.610143],\n",
      "       [-9.265321]], dtype=float32)]\n",
      "6900 0.030232139 [array([[-4.350143 , -6.1254463],\n",
      "       [-4.3644567, -6.2248487]], dtype=float32), array([[ 8.665175],\n",
      "       [-9.313123]], dtype=float32)]\n",
      "7000 0.029384239 [array([[-4.371939 , -6.139919 ],\n",
      "       [-4.386091 , -6.2379074]], dtype=float32), array([[ 8.718745],\n",
      "       [-9.359815]], dtype=float32)]\n",
      "7100 0.028580494 [array([[-4.393081 , -6.154007 ],\n",
      "       [-4.407078 , -6.2506375]], dtype=float32), array([[ 8.770921],\n",
      "       [-9.405447]], dtype=float32)]\n",
      "7200 0.027817579 [array([[-4.4136043, -6.167729 ],\n",
      "       [-4.4274507, -6.263053 ]], dtype=float32), array([[ 8.821776],\n",
      "       [-9.450067]], dtype=float32)]\n",
      "7300 0.027092557 [array([[-4.433541 , -6.181103 ],\n",
      "       [-4.4472413, -6.2751694]], dtype=float32), array([[ 8.871371],\n",
      "       [-9.493712]], dtype=float32)]\n",
      "7400 0.026402805 [array([[-4.452921 , -6.1941457],\n",
      "       [-4.4664793, -6.2869997]], dtype=float32), array([[ 8.9197645],\n",
      "       [-9.536425 ]], dtype=float32)]\n",
      "7500 0.025745776 [array([[-4.471771 , -6.206871 ],\n",
      "       [-4.4851933, -6.298557 ]], dtype=float32), array([[ 8.967011],\n",
      "       [-9.578244]], dtype=float32)]\n",
      "7600 0.025119428 [array([[-4.490117 , -6.219293 ],\n",
      "       [-4.503407 , -6.3098507]], dtype=float32), array([[ 9.013165],\n",
      "       [-9.619203]], dtype=float32)]\n",
      "7700 0.024521574 [array([[-4.5079823, -6.2314262],\n",
      "       [-4.521146 , -6.320894 ]], dtype=float32), array([[ 9.05827 ],\n",
      "       [-9.659337]], dtype=float32)]\n",
      "7800 0.02395037 [array([[-4.525391, -6.24328 ],\n",
      "       [-4.53843 , -6.331697]], dtype=float32), array([[ 9.102373],\n",
      "       [-9.698677]], dtype=float32)]\n",
      "7900 0.023404185 [array([[-4.5423636, -6.254869 ],\n",
      "       [-4.555281 , -6.342269 ]], dtype=float32), array([[ 9.145516],\n",
      "       [-9.737252]], dtype=float32)]\n",
      "8000 0.022881366 [array([[-4.5589175, -6.2662024],\n",
      "       [-4.571719 , -6.352618 ]], dtype=float32), array([[ 9.187737],\n",
      "       [-9.77509 ]], dtype=float32)]\n",
      "8100 0.022380566 [array([[-4.5750737, -6.277292 ],\n",
      "       [-4.587761 , -6.3627543]], dtype=float32), array([[ 9.229079],\n",
      "       [-9.812221]], dtype=float32)]\n",
      "8200 0.02190043 [array([[-4.5908484, -6.288146 ],\n",
      "       [-4.603425 , -6.3726845]], dtype=float32), array([[ 9.269571],\n",
      "       [-9.848667]], dtype=float32)]\n",
      "8300 0.021439629 [array([[-4.606258 , -6.298775 ],\n",
      "       [-4.6187263, -6.382417 ]], dtype=float32), array([[ 9.30925 ],\n",
      "       [-9.884455]], dtype=float32)]\n",
      "8400 0.020997237 [array([[-4.6213155, -6.3091855],\n",
      "       [-4.633679 , -6.39196  ]], dtype=float32), array([[ 9.3481455],\n",
      "       [-9.919606 ]], dtype=float32)]\n",
      "8500 0.020572033 [array([[-4.636038 , -6.319388 ],\n",
      "       [-4.648299 , -6.4013205]], dtype=float32), array([[ 9.386288],\n",
      "       [-9.954142]], dtype=float32)]\n",
      "8600 0.020163145 [array([[-4.6504374, -6.32939  ],\n",
      "       [-4.6626   , -6.410502 ]], dtype=float32), array([[ 9.423704],\n",
      "       [-9.988083]], dtype=float32)]\n",
      "8700 0.019769631 [array([[-4.664527 , -6.3391967],\n",
      "       [-4.6765924, -6.4195137]], dtype=float32), array([[  9.460422],\n",
      "       [-10.021449]], dtype=float32)]\n",
      "8800 0.019390639 [array([[-4.6783185, -6.348816 ],\n",
      "       [-4.6902895, -6.428361 ]], dtype=float32), array([[  9.496465],\n",
      "       [-10.05426 ]], dtype=float32)]\n",
      "8900 0.019025436 [array([[-4.691824 , -6.358256 ],\n",
      "       [-4.7037024, -6.43705  ]], dtype=float32), array([[  9.531855],\n",
      "       [-10.086533]], dtype=float32)]\n",
      "9000 0.01867335 [array([[-4.705054 , -6.367523 ],\n",
      "       [-4.7168407, -6.445584 ]], dtype=float32), array([[  9.566619],\n",
      "       [-10.118283]], dtype=float32)]\n",
      "9100 0.018333554 [array([[-4.718017 , -6.376619 ],\n",
      "       [-4.729717 , -6.4539685]], dtype=float32), array([[  9.600778 ],\n",
      "       [-10.1495285]], dtype=float32)]\n",
      "9200 0.018005542 [array([[-4.730725 , -6.385554 ],\n",
      "       [-4.742338 , -6.4622097]], dtype=float32), array([[  9.63435 ],\n",
      "       [-10.180285]], dtype=float32)]\n",
      "9300 0.017688673 [array([[-4.743186 , -6.3943324],\n",
      "       [-4.754715 , -6.470312 ]], dtype=float32), array([[  9.667356],\n",
      "       [-10.210567]], dtype=float32)]\n",
      "9400 0.017382424 [array([[-4.755409 , -6.4029565],\n",
      "       [-4.7668552, -6.478278 ]], dtype=float32), array([[  9.699814],\n",
      "       [-10.240389]], dtype=float32)]\n",
      "9500 0.017086277 [array([[-4.7674017, -6.4114347],\n",
      "       [-4.7787685, -6.4861135]], dtype=float32), array([[  9.731737],\n",
      "       [-10.269763]], dtype=float32)]\n",
      "9600 0.0167998 [array([[-4.779174 , -6.4197702],\n",
      "       [-4.7904587, -6.493824 ]], dtype=float32), array([[  9.763149],\n",
      "       [-10.298703]], dtype=float32)]\n",
      "9700 0.016522424 [array([[-4.790729 , -6.4279685],\n",
      "       [-4.8019385, -6.501409 ]], dtype=float32), array([[  9.7940645],\n",
      "       [-10.327221 ]], dtype=float32)]\n",
      "9800 0.016253807 [array([[-4.802078 , -6.436031 ],\n",
      "       [-4.8132105, -6.5088773]], dtype=float32), array([[  9.824496],\n",
      "       [-10.355331]], dtype=float32)]\n",
      "9900 0.015993498 [array([[-4.813226 , -6.4439635],\n",
      "       [-4.8242836, -6.5162272]], dtype=float32), array([[  9.854458],\n",
      "       [-10.383042]], dtype=float32)]\n",
      "10000 0.015741188 [array([[-4.8241796, -6.451773 ],\n",
      "       [-4.8351645, -6.523467 ]], dtype=float32), array([[  9.883966],\n",
      "       [-10.410365]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[0.01225582]\n",
      " [0.9859859 ]\n",
      " [0.9858948 ]\n",
      " [0.02206718]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0],    [1],    [1],    [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W1= tf.Variable(tf.random_normal([2, 2]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y *tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize Tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-17fdcc59c5a5>, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-17fdcc59c5a5>\"\u001b[1;36m, line \u001b[1;32m43\u001b[0m\n\u001b[1;33m    :\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# W1 =tf.Variable(tf.random_uniform([2, 5], -1.0, 1.0), name = 'weight1')\n",
    "# W2 =tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'weight2')\n",
    "# W3 =tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'weight3')\n",
    "# W4 =tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'weight4')\n",
    "# W5 =tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'weight5')\n",
    "# W6 =tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'weight6')\n",
    "# W7 =tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'weight7')\n",
    "# W8 =tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'weight8')\n",
    "# W9 =tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'weight9')\n",
    "# W10 =tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'weight10')\n",
    "# W11 =tf.Variable(tf.random_uniform([5, 1], -1.0, 1.0), name = 'weight11')\n",
    "\n",
    "# b1 = tf.Variable(tf.zeros([5]), name = 'Bias1')\n",
    "# b2 = tf.Variable(tf.zeros([5]), name = 'Bias2')\n",
    "# b3 = tf.Variable(tf.zeros([5]), name = 'Bias3')\n",
    "# b4 = tf.Variable(tf.zeros([5]), name = 'Bias4')\n",
    "# b5 = tf.Variable(tf.zeros([5]), name = 'Bias5')\n",
    "# b6 = tf.Variable(tf.zeros([5]), name = 'Bias6')\n",
    "# b7 = tf.Variable(tf.zeros([5]), name = 'Bias7')\n",
    "# b8 = tf.Variable(tf.zeros([5]), name = 'Bias8')\n",
    "# b9 = tf.Variable(tf.zeros([5]), name = 'Bias9')\n",
    "# b10 = tf.Variable(tf.zeros([5]), name = 'Bias10')\n",
    "# b11 = tf.Variable(tf.zeros([1]), name = 'Bias11')\n",
    "\n",
    "# # Our hypothesis\n",
    "# L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "# L2 = tf.sigmoid(tf.matmul(L1, W2) + b2)\n",
    "# L3 = tf.sigmoid(tf.matmul(L2, W3) + b3)\n",
    "# L4 = tf.sigmoid(tf.matmul(L3, W4) + b4)\n",
    "# L5 = tf.sigmoid(tf.matmul(L4, W5) + b5)\n",
    "# L6 = tf.sigmoid(tf.matmul(L5, W6) + b6)\n",
    "# L7 = tf.sigmoid(tf.matmul(L6, W7) + b7)\n",
    "# L8 = tf.sigmoid(tf.matmul(L7, W8) + b8)\n",
    "# L9 = tf.sigmoid(tf.matmul(L8, W9) + b9)\n",
    "# L10 = tf.sigmoid(tf.matmul(L9, W10) + b10)\n",
    "\n",
    "# hypothesis = tf.sigmoid(tf.matmul(L10, W11) + b11)\n",
    "\n",
    "\n",
    "# # Hypothesis를 텐서보드로 시각화\n",
    "# with tf.name_scope(\"layer1\") as scope:\n",
    "#     L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "# :\n",
    "# :\n",
    "# :\n",
    "# with tf.name_scope(\"last\") as scope:\n",
    "#     hypothesis= tf.sigmoid(tf.matul(L10,W11) +b11)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ReLU \n",
    "# L1 = tf.nn.relu(tf.matmul(X, W1)+ b1)\n",
    "\n",
    "# #Sigmoid\n",
    "# L1 = tf.sigmoid(tf.matmul(X, W1) + b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n",
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 1)\n",
      "conv2d_img.shape (1, 2, 2, 1)\n",
      "[[12. 16.]\n",
      " [24. 28.]]\n",
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 1)\n",
      "conv2d_img.shape (1, 3, 3, 1)\n",
      "[[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "image = np.array([[[[1],[2],[3]],\n",
    "                   [[4],[5],[6]],\n",
    "                   [[7],[8],[9]]]], dtype=np.float32)\n",
    "print(image.shape)\n",
    "plt.imshow(image.reshape(3,3), cmap = 'Greys')\n",
    "\n",
    "\n",
    "# weight값과stride 주고 통과시키기\n",
    "\n",
    "print(\"image.shape\", image.shape)\n",
    "weight = tf.constant([[[[1.]],[[1.]]],\n",
    "                      [[[1.]],[[1.]]]])\n",
    "print(\"weight.shape\", weight.shape)\n",
    "conv2d = tf.nn.conv2d(image, weight, strides = [1, 1, 1,1],padding='VALID')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(2,2))\n",
    "    plt.subplot(1,2,i+1),plt.imshow(one_img.reshape(2,2),cmap='gray')\n",
    "    \n",
    "    \n",
    "#padding 이용하기\n",
    "print(\"image.shape\", image.shape)\n",
    "\n",
    "weight = tf.constant([[[[1.]],[[1.]]],\n",
    "                      [[[1.]],[[1.]]]])\n",
    "print(\"weight.shape\", weight.shape)\n",
    "conv2d = tf.nn.conv2d(image, weight, strides= [1,1,1,1], padding='SAME')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(3, 3))\n",
    "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(3,3), cmap=\"gray\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n",
      "[[[[4.]\n",
      "   [4.]]\n",
      "\n",
      "  [[2.]\n",
      "   [2.]]]]\n"
     ]
    }
   ],
   "source": [
    "# Max Pooling\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "image = np.array([[[[4],[3]],\n",
    "                   [[2],[1]]]], dtype = np.float32)\n",
    "pool = tf.nn.max_pool(image, ksize = [1, 2, 3, 1],\n",
    "                     strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "print(pool.shape)\n",
    "print(pool.eval())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
